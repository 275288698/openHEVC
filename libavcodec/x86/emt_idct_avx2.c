#include "libavcodec/x86/hevcdsp.h"
#include "libavcodec/hevc_amt_defs.h"

#if HAVE_AVX2
#include <immintrin.h>
#endif

#if HAVE_SSE2
#include <emmintrin.h>
#endif

#include <time.h>
#include "syscall.h"
#include "stdint.h"
#include <smmintrin.h>


#include "libavcodec/bit_depth_template.c"


#if OHCONFIG_AMT
#if BIT_DEPTH < 9
//DECLARE_ALIGNED(16, static const int16_t, DCT_II_4x4[4][4]) =
//{
//    { 256,  256,  256,  256 },
//    { 334,  139, -139, -334 },
//    { 256, -256, -256,  256 },
//    { 139, -334,  334, -139 }
//};

//DECLARE_ALIGNED(16, static const int16_t, DCT_V_4x4[4][4]) =
//{
//    {194,  274,  274,  274},
//    {274,  241,  -86, -349},
//    {274,  -86, -349,  241},
//    {274, -349,  241,  -86}
//};

//DECLARE_ALIGNED(16, static const int16_t, DCT_VIII_4x4[4][4]) =
//{
//    {  336,  296,  219,  117 },
//    {  296,    0, -296, -296 },
//    {  219, -296, -117,  336 },
//    {  117, -296,  336, -219 }
//};

//DECLARE_ALIGNED(16, static const int16_t, DST_I_4x4[4][4]) =
//{
//    {  190,  308,  308,  190 },
//    {  308,  190, -190, -308 },
//    {  308, -190, -190,  308 },
//    {  190, -308,  308, -190 }
//};

//DECLARE_ALIGNED(16, static const int16_t, DST_VII_4x4[4][4]) =
//{
//    {  117,  219,  296,  336 },
//    {  296,  296,    0, -296 },
//    {  336, -117, -296,  219 },
//    {  219, -336,  296, -117 }
//};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_II_8x8_per_CG[4][16]) ={
    {  256,  355,  334,  301,  256,  301,  139,  -71,  256,  201, -139, -355,  256,   71, -334, -201 },
    {  256,  201,  139,   71, -256, -355, -334, -201, -256,   71,  334,  301,  256,  301, -139, -355 },
    {  256,  -71, -334,  201,  256, -201, -139,  355,  256, -301,  139,   71,  256, -355,  334, -301 },
    {  256, -301, -139,  355, -256,  -71,  334, -301, -256,  355, -334,  201,  256, -201,  139,  -71,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_V_8x8_per_CG[4][16]) ={
    {  187,  264,  264,  264,  264,  342,  250,  116,  264,  250,  -39, -303,  264,  116, -303, -303 },
    {  264,  264,  264,  264,  -39, -187, -303, -366, -366, -187,  116,  342,  116,  374,  116, -303 },
    {  264,  -39, -366,  116,  264, -187, -187,  374,  264, -303,  116,  116,  264, -366,  342, -303 },
    {  342, -187, -303,  250, -187, -187,  374, -187, -303,  374, -303,  116,  250, -187,  116,  -39,},
};

DECLARE_ALIGNED(32, static const int16_t, TR_DCT_VIII_8x8_per_CG[4][16]) =
{
    {  350,  338,  314,  280,  338,  237,   65, -127,  314,   65, -237, -350, 280, -127, -350,  -65 },
    {  237,  185,  127,   65, -280, -350, -314, -185, -185,  127,  338,  280, 314,  237, -185, -338 },
    {  237, -280, -185,  314,  185, -350,  127,  237,  127, -314,  338, -185,  65, -185,  280, -338 },
    {  127, -338,  -65,  350, -338,   65,  280, -314,  -65,  280, -350,  237, 350, -314,  237, -127 },
};

DECLARE_ALIGNED(16, static const int16_t, TR_DST_I_8x8_per_CG[4][16]) ={
    {  117,  219,  296,  336,  219,  336,  296,  117,  296,  296,    0, -296,  336,  117, -296, -219,},
    {  336,  296,  219,  117, -117, -296, -336, -219, -296,    0,  296,  296,  219,  296, -117, -336,},
    {  336, -117, -296,  219,  296, -296,    0,  296,  219, -336,  296, -117,  117, -219,  296, -336,},
    {  219, -296, -117,  336, -296,    0,  296, -296, -117,  296, -336,  219,  336, -296,  219, -117,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DST_VII_8x8_per_CG[4][16]) ={
    {   65,  185,  280,  338,  127,  314,  338,  185,  185,  350,  127, -237,  237,  280, -185, -314 },
    {  350,  314,  237,  127,  -65, -280, -350, -237, -338,  -65,  280,  314,  127,  338,  -65, -350 },
    {  280,  127, -350,   65,  314,  -65, -237,  350,  338, -237,   65,  127,  350, -338,  314, -280 },
    {  314, -237, -185,  338, -185, -127,  338, -280, -280,  350, -314,  185,  237, -185,  127,  -65,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_II_16x16_per_CG[16][16]) ={
    {  256,  360,  355,  346,  256,  346,  301,  230,  256,  319,  201,   35,  256,  280,   71, -171,},
    {  334,  319,  301,  280,  139,   35,  -71, -171, -139, -280, -355, -346, -334, -346, -201,   35,},
    {  256,  230,  201,  171, -256, -319, -355, -360, -256, -105,   71,  230,  256,  360,  301,  105,},
    {  139,  105,   71,   35, -334, -280, -201, -105,  334,  360,  301,  171, -139, -319, -355, -230,},
    {  256,  230,  -71, -319,  256,  171, -201, -360,  256,  105, -301, -280,  256,   35, -355, -105,},
    { -334, -105,  201,  360, -139,  230,  355,  105,  139,  360,   71, -319,  334,  171, -301, -230,},
    {  256,  -35, -301, -346, -256, -346,  -71,  280, -256,  171,  355,   35,  256,  280, -201, -319,},
    { -139,  171,  355,  280,  334,   35, -301, -319, -334, -230,  201,  346,  139,  346,  -71, -360,},
    {  256,  -35, -355,  105,  256, -105, -301,  280,  256, -171, -201,  360,  256, -230,  -71,  319,},
    {  334, -171, -301,  230,  139, -360,   71,  319, -139, -230,  355, -105, -334,  105,  201, -360,},
    {  256, -280, -201,  319, -256, -171,  355,  -35, -256,  346,  -71, -280,  256,   35, -301,  346,},
    {  139, -346,  -71,  360, -334,  230,  201, -346,  334,  -35, -301,  319, -139, -171,  355, -280,},
    {  256, -280,   71,  171,  256, -319,  201,  -35,  256, -346,  301, -230,  256, -360,  355, -346,},
    { -334,  346, -201,  -35, -139,  280, -355,  346,  139,  -35,  -71,  171,  334, -319,  301, -280,},
    {  256, -360,  301, -105, -256,  105,   71, -230, -256,  319, -355,  360,  256, -230,  201, -171,},
    { -139,  319, -355,  230,  334, -360,  301, -171, -334,  280, -201,  105,  139, -105,   71,  -35,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_V_16x16_per_CG[16][16]) ={
    {  184,  260,  260,  260,  260,  360,  338,  302,  260,  338,  253,  128,  260,  302,  128,  -92,},
    {  260,  260,  260,  260,  253,  195,  128,   56,  -19, -162, -279, -351, -279, -366, -322, -162,},
    {  260,  260,  260,  260,  -19,  -92, -162, -225, -366, -322, -225,  -92,   56,  253,  360,  338,},
    {  260,  260,  260,  260, -279, -322, -351, -366,   56,  195,  302,  360,  195,  -19, -225, -351,},
    {  260,  253,  -19, -279,  260,  195, -162, -366,  260,  128, -279, -322,  260,   56, -351, -162,},
    { -366, -225,   56,  302, -225,  128,  360,  253,   56,  360,  195, -225,  302,  253, -225, -322,},
    {  360,  195,  -92, -322,  -92, -351, -279,   56, -351,  -19,  338,  253,  128,  360,  -19, -366,},
    { -351, -162,  128,  338,  338,  302,  -19, -322, -162, -366,  -92,  302,  -92,  338,  195, -279,},
    {  260,  -19, -366,   56,  260,  -92, -322,  253,  260, -162, -225,  360,  260, -225,  -92,  338,},
    {  360,  -92, -351,  128,  195, -351,  -19,  360,  -92, -279,  338,  -19, -322,   56,  253, -366,},
    {  338, -162, -322,  195, -162, -279,  302,  128, -322,  302,   56, -351,  195,  128, -351,  302,},
    {  302, -225, -279,  253, -366,   56,  338, -225,  253,  128, -366,  195,  -19, -279,  360, -162,},
    {  260, -279,   56,  195,  260, -322,  195,  -19,  260, -351,  302, -225,  260, -366,  360, -351,},
    { -351,  338, -162,  -92, -162,  302, -366,  338,  128,  -19,  -92,  195,  338, -322,  302, -279,},
    {  302, -366,  253,  -19, -225,   56,  128, -279, -279,  338, -366,  360,  253, -225,  195, -162,},
    { -225,  360, -322,  128,  360, -351,  253,  -92, -322,  253, -162,   56,  128,  -92,   56,  -19,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_VIII_16x16_per_CG[16][16]) ={
    {  356,  353,  346,  337,  353,  324,  269,  193,  346,  269,  133,  -34,  337,  193,  -34, -246,},
    {  324,  309,  290,  269,  100,    0, -100, -193, -193, -309, -356, -324, -353, -309, -133,  100,},
    {  246,  220,  193,  163, -269, -324, -353, -353, -220,  -67,  100,  246,  290,  356,  269,   67,},
    {  133,  100,   67,   34, -324, -269, -193, -100,  337,  353,  290,  163, -163, -324, -346, -220,},
    {  324,  100, -193, -353,  309,    0, -309, -309,  290, -100, -356, -133,  269, -193, -324,  100,},
    { -269,    0,  269,  353,    0,  309,  309,    0,  269,  309,  -67, -353,  353,    0, -353, -100,},
    {  193, -100, -324, -324, -309, -309,    0,  309, -163,  246,  324,  -34,  324,  193, -269, -269,},
    { -100,  193,  353,  269,  309,    0, -309, -309, -346, -193,  220,  337,  193,  324, -100, -353,},
    {  246, -269, -220,  290,  220, -324,  -67,  356,  193, -353,  100,  269,  163, -353,  246,   67,},
    {  193, -309, -163,  324, -100, -309,  246,  193, -324,    0,  324, -269, -324,  309,  -34, -269,},
    {  133, -337, -100,  346, -337,  -34,  353, -133, -100,  353, -193, -193,  346, -133, -193,  356,},
    {   67, -353,  -34,  356, -290,  269,  163, -346,  353, -100, -269,  324, -220, -100,  337, -290,},
    {  133, -324,  337, -163,  100, -269,  353, -324,   67, -193,  290, -346,   34, -100,  163, -220,},
    { -100,  309, -346,  193,  193,    0, -193,  324,  353, -309,  220, -100,  269, -309,  337, -353,},
    {   67, -290,  353, -220, -353,  269, -100, -100,  -34,  163, -269,  337,  356, -346,  324, -290,},
    {  -34,  269, -356,  246,  269, -353,  324, -193, -356,  324, -246,  133,  246, -193,  133,  -67,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DST_I_16x16_per_CG[16][16]) ={
    {   65,  127,  185,  237,  127,  237,  314,  350,  185,  314,  350,  280,  237,  350,  280,   65,},
    {  280,  314,  338,  350,  338,  280,  185,   65,  127,  -65, -237, -338, -185, -338, -314, -127,},
    {  350,  338,  314,  280,  -65, -185, -280, -338, -338, -237,  -65,  127,  127,  314,  338,  185,},
    {  237,  185,  127,   65, -350, -314, -237, -127,  280,  350,  314,  185,  -65, -280, -350, -237,},
    {  280,  338,  127, -185,  314,  280,  -65, -338,  338,  185, -237, -314,  350,   65, -338, -127,},
    { -350, -237,   65,  314, -237,  127,  350,  185,   65,  350,  127, -280,  314,  185, -280, -237,},
    {  314,   65, -237, -350, -185, -350, -127,  237, -280,  127,  350,   65,  237,  280, -185, -314,},
    { -185,  127,  338,  280,  338,   65, -280, -314, -314, -237,  185,  338,  127,  338,  -65, -350,},
    {  350,  -65, -338,  127,  338, -185, -237,  314,  314, -280,  -65,  338,  280, -338,  127,  185,},
    {  314, -185, -280,  237,   65, -350,  127,  280, -237, -127,  350, -185, -350,  237,   65, -314,},
    {  237, -280, -185,  314, -280, -127,  350,  -65, -185,  350, -127, -237,  314,  -65, -237,  350,},
    {  127, -338,  -65,  350, -314,  237,  185, -338,  338,  -65, -280,  314, -185, -127,  338, -280,},
    {  237, -350,  280,  -65,  185, -314,  350, -280,  127, -237,  314, -350,   65, -127,  185, -237,},
    { -185,  338, -314,  127,  127,   65, -237,  338,  338, -280,  185,  -65,  280, -314,  338, -350,},
    {  127, -314,  338, -185, -338,  237,  -65, -127,  -65,  185, -280,  338,  350, -338,  314, -280,},
    {  -65,  280, -350,  237,  280, -350,  314, -185, -350,  314, -237,  127,  237, -185,  127,  -65,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DST_VII_16x16_per_CG[16][16]) ={
    {   34,  100,  163,  220,   67,  193,  290,  346,  100,  269,  353,  324,  133,  324,  337,  163,},
    {  269,  309,  337,  353,  353,  309,  220,  100,  193,    0, -193, -324, -100, -309, -346, -193,},
    {  356,  346,  324,  290,  -34, -163, -269, -337, -353, -269, -100,  100,   67,  290,  353,  220,},
    {  246,  193,  133,   67, -356, -324, -246, -133,  269,  353,  324,  193,  -34, -269, -356, -246,},
    {  163,  353,  246,  -67,  193,  353,  100, -269,  220,  324,  -67, -356,  246,  269, -220, -290,},
    { -324, -309,  -34,  269, -324,    0,  324,  269, -100,  309,  246, -193,  193,  309, -163, -324,},
    {  346,  133, -193, -356, -100, -353, -193,  193, -337,   34,  353,  133,  133,  337, -100, -346,},
    { -220,  100,  337,  290,  353,  100, -269, -324, -290, -269,  163,  346,   67,  353,  -34, -356,},
    {  269,  193, -324, -100,  290,  100, -356,  133,  309,    0, -309,  309,  324, -100, -193,  353,},
    {  353,    0, -353,  100,  269, -309,  -67,  353,    0, -309,  309,    0, -269,    0,  269, -353,},
    {  324, -193, -269,  269, -163, -246,  324,   34, -309,  309,    0, -309,  193,  100, -324,  324,},
    {  193, -324, -100,  353, -346,  193,  220, -337,  309,    0, -309,  309, -100, -193,  353, -269,},
    {  337, -193,  -34,  246,  346, -269,  133,   34,  353, -324,  269, -193,  356, -353,  346, -337,},
    { -353,  309, -133, -100, -193,  309, -356,  324,  100,    0, -100,  193,  324, -309,  290, -269,},
    {  290, -356,  269,  -67, -220,   67,  100, -246, -269,  324, -353,  353,  246, -220,  193, -163,},
    { -163,  324, -346,  220,  337, -353,  290, -163, -324,  269, -193,  100,  133, -100,   67,  -34,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_II_32x32_per_CG[64][16]) ={
    {  256,  362,  360,  358,  256,  358,  346,  327,  256,  351,  319,  268,  256,  341,  280,  186,},
    {  355,  351,  346,  341,  301,  268,  230,  186,  201,  122,   35,  -53,   71,  -53, -171, -268,},
    {  334,  327,  319,  311,  139,   88,   35,  -18, -139, -216, -280, -327, -334, -362, -346, -291,},
    {  301,  291,  280,  268,  -71, -122, -171, -216, -355, -362, -346, -311, -201,  -88,   35,  155,},
    {  256,  243,  230,  216, -256, -291, -319, -341, -256, -186, -105,  -18,  256,  327,  360,  351,},
    {  201,  186,  171,  155, -355, -362, -360, -351,   71,  155,  230,  291,  301,  216,  105,  -18,},
    {  139,  122,  105,   88, -334, -311, -280, -243,  334,  358,  360,  341, -139, -243, -319, -358,},
    {   71,   53,   35,   18, -201, -155, -105,  -53,  301,  243,  171,   88, -355, -311, -230, -122,},
    {  256,  327,  230,   88,  256,  311,  171,  -18,  256,  291,  105, -122,  256,  268,   35, -216,},
    {  -71, -216, -319, -362, -201, -327, -360, -291, -301, -362, -280,  -88, -355, -311, -105,  155,},
    { -334, -243, -105,   53, -139,   53,  230,  341,  139,  311,  360,  268,  334,  341,  171,  -88,},
    {  201,  311,  360,  341,  355,  268,  105,  -88,   71, -155, -319, -358, -301, -358, -230,   18,},
    {  256,  122,  -35, -186, -256, -351, -346, -243, -256,  -53,  171,  327,  256,  362,  280,   53,},
    { -301, -358, -346, -268,  -71,  122,  280,  358,  355,  243,   35, -186, -201, -351, -319, -122,},
    { -139,   18,  171,  291,  334,  216,   35, -155, -334, -351, -230,  -18,  139,  327,  346,  186,},
    {  355,  351,  280,  155, -301, -362, -319, -186,  201,  341,  346,  216,  -71, -291, -360, -243,},
    {  256,  243,  -35, -291,  256,  216, -105, -341,  256,  186, -171, -362,  256,  155, -230, -351,},
    { -355, -186,  105,  327, -301,  -18,  280,  351, -201,  155,  360,  216,  -71,  291,  319,  -18,},
    {  334,  122, -171, -351,  139, -186, -360, -243, -139, -358, -230,  122, -334, -268,  105,  358,},
    { -301,  -53,  230,  362,   71,  327,  319,   53,  355,  243, -105, -351,  201, -186, -360, -122,},
    {  256,  -18, -280, -358, -256, -358, -171,  155, -256,   88,  346,  268,  256,  341,   35, -311,},
    { -201,   88,  319,  341,  355,  268,  -35, -311,  -71, -341, -280,   53, -301,   53,  346,  243,},
    {  139, -155, -346, -311, -334,  -88,  230,  362,  334,  291,  -35, -327, -139, -362, -171,  216,},
    {  -71,  216,  360,  268,  201, -122, -346, -291, -301,   18,  319,  311,  355,   88, -280, -327,},
    {  256,  122, -280, -311,  256,   88, -319, -243,  256,   53, -346, -155,  256,   18, -360,  -53,},
    {   71,  358,  171, -243,  201,  341,  -35, -358,  301,  243, -230, -311,  355,   88, -346, -122,},
    { -334,   18,  346,  216, -139,  291,  280, -155,  139,  351,  -35, -362,  334,  155, -319, -186,},
    { -201, -351,  -35,  327, -355,  -18,  346,  186,  -71,  341,  171, -291,  301,  216, -280, -243,},
    {  256, -155, -360,  -88, -256, -311,  105,  362, -256,  216,  319, -122,  256,  268, -230, -291,},
    {  301,  291, -105, -362,   71, -327, -230,  216, -355,   18,  360,   88,  201,  311, -171, -327,},
    { -139,  268,  319,  -53,  334,  -53, -360, -122, -334, -186,  280,  268,  139,  341, -105, -351,},
    { -355, -186,  230,  341,  301,  268, -171, -351, -201, -327,  105,  358,   71,  358,  -35, -362,},
    {  256,  -18, -360,   53,  256,  -53, -346,  155,  256,  -88, -319,  243,  256, -122, -280,  311,},
    {  355,  -88, -346,  122,  301, -243, -230,  311,  201, -341,  -35,  358,   71, -358,  171,  243,},
    {  334, -155, -319,  186,  139, -351,  -35,  362, -139, -291,  280,  155, -334,  -18,  346, -216,},
    {  301, -216, -280,  243,  -71, -341,  171,  291, -355,   18,  346, -186, -201,  351,  -35, -327,},
    {  256, -268, -230,  291, -256, -216,  319,  122, -256,  311,  105, -362,  256,  155, -360,   88,},
    {  201, -311, -171,  327, -355,  -18,  360,  -88,   71,  327, -230, -216,  301, -291, -105,  362,},
    {  139, -341, -105,  351, -334,  186,  280, -268,  334,   53, -360,  122, -139, -268,  319,   53,},
    {   71, -358,  -35,  362, -201,  327,  105, -358,  301, -268, -171,  351, -355,  186,  230, -341,},
    {  256, -155, -230,  351,  256, -186, -171,  362,  256, -216, -105,  341,  256, -243,  -35,  291,},
    {  -71, -291,  319,   18, -201, -155,  360, -216, -301,   18,  280, -351, -355,  186,  105, -327,},
    { -334,  268,  105, -358, -139,  358, -230, -122,  139,  186, -360,  243,  334, -122, -171,  351,},
    {  201,  186, -360,  122,  355, -243, -105,  351,   71, -327,  319,  -53, -301,   53,  230, -362,},
    {  256, -341,   35,  311, -256,  -88,  346, -268, -256,  358, -171, -155,  256,   18, -280,  358,},
    { -301,  -53,  346, -243,  -71,  341, -280,  -53,  355, -268,  -35,  311, -201,  -88,  319, -341,},
    { -139,  362, -171, -216,  334, -291,  -35,  327, -334,   88,  230, -362,  139,  155, -346,  311,},
    {  355,  -88, -280,  327, -301,  -18,  319, -311,  201,  122, -346,  291,  -71, -216,  360, -268,},
    {  256, -268,   35,  216,  256, -291,  105,  122,  256, -311,  171,   18,  256, -327,  230,  -88,},
    { -355,  311, -105, -155, -301,  362, -280,   88, -201,  327, -360,  291,  -71,  216, -319,  362,},
    {  334, -341,  171,   88,  139, -311,  360, -268, -139,  -53,  230, -341, -334,  243, -105,  -53,},
    { -301,  358, -230,  -18,   71,  155, -319,  358,  355, -268,  105,   88,  201, -311,  360, -341,},
    {  256, -362,  280,  -53, -256,   53,  171, -327, -256,  351, -346,  243,  256, -122,  -35,  186,},
    { -201,  351, -319,  122,  355, -243,   35,  186,  -71, -122,  280, -358, -301,  358, -346,  268,},
    {  139, -327,  346, -186, -334,  351, -230,   18,  334, -216,   35,  155, -139,  -18,  171, -291,},
    {  -71,  291, -360,  243,  201, -341,  346, -216, -301,  362, -319,  186,  355, -351,  280, -155,},
    {  256, -341,  280, -186,  256, -351,  319, -268,  256, -358,  346, -327,  256, -362,  360, -358,},
    {   71,   53, -171,  268,  201, -122,   35,   53,  301, -268,  230, -186,  355, -351,  346, -341,},
    { -334,  362, -346,  291, -139,  216, -280,  327,  139,  -88,   35,   18,  334, -327,  319, -311,},
    { -201,   88,   35, -155, -355,  362, -346,  311,  -71,  122, -171,  216,  301, -291,  280, -268,},
    {  256, -327,  360, -351, -256,  186, -105,   18, -256,  291, -319,  341,  256, -243,  230, -216,},
    {  301, -216,  105,   18,   71, -155,  230, -291, -355,  362, -360,  351,  201, -186,  171, -155,},
    { -139,  243, -319,  358,  334, -358,  360, -341, -334,  311, -280,  243,  139, -122,  105,  -88,},
    { -355,  311, -230,  122,  301, -243,  171,  -88, -201,  155, -105,   53,   71,  -53,   35,  -18,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_V_32x32_per_CG[64][16]) ={
    {  182,  258,  258,  258,  258,  363,  358,  349,  258,  358,  336,  301,  258,  349,  301,  228,},
    {  258,  258,  258,  258,  336,  320,  301,  280,  255,  198,  133,   63,  133,   27,  -81, -182,},
    {  258,  258,  258,  258,  255,  228,  198,  166,   -9,  -81, -150, -213, -267, -329, -361, -361,},
    {  258,  258,  258,  258,  133,   99,   63,   27, -267, -311, -343, -361, -329, -267, -182,  -81,},
    {  258,  258,  258,  258,   -9,  -45,  -81, -116, -364, -354, -329, -291,   27,  133,  228,  301,},
    {  258,  258,  258,  258, -150, -182, -213, -241, -241, -182, -116,  -45,  349,  365,  349,  301,},
    {  258,  258,  258,  258, -267, -291, -311, -329,   27,   99,  166,  228,  228,  133,   27,  -81,},
    {  258,  258,  258,  258, -343, -354, -361, -364,  280,  320,  349,  363, -182, -267, -329, -361,},
    {  258,  336,  255,  133,  258,  320,  198,   27,  258,  301,  133,  -81,  258,  280,   63, -182,},
    {   -9, -150, -267, -343, -150, -291, -361, -343, -267, -361, -329, -182, -343, -343, -182,   63,},
    { -364, -329, -241, -116, -241,  -81,   99,  255,   27,  228,  349,  349,  280,  365,  280,   63,},
    {   27,  166,  280,  349,  349,  358,  280,  133,  228,   27, -182, -329, -182, -343, -343, -182,},
    {  363,  320,  228,   99,  -45, -213, -329, -364, -361, -267,  -81,  133,   63,  280,  365,  280,},
    {  -45, -182, -291, -354, -311, -182,   -9,  166,  301,  365,  301,  133,   63, -182, -343, -343,},
    { -361, -311, -213,  -81,  301,  363,  336,  228,  -81, -267, -361, -329, -182,   63,  280,  365,},
    {   63,  198,  301,  358,   63, -116, -267, -354, -182,   27,  228,  349,  280,   63, -182, -343,},
    {  258,  255,   -9, -267,  258,  228,  -81, -329,  258,  198, -150, -361,  258,  166, -213, -361,},
    { -364, -241,   27,  280, -329,  -81,  228,  365, -241,   99,  349,  280, -116,  255,  349,   63,},
    {  363,  228,  -45, -291,  228,  -81, -329, -329,  -45, -329, -311,   -9, -291, -329,   -9,  320,},
    { -361, -213,   63,  301,  -81,  228,  365,  228,  301,  336,   63, -267,  301,  -45, -343, -267,},
    {  358,  198,  -81, -311,  -81, -329, -329,  -81, -354, -116,  228,  363,   99,  358,  228, -150,},
    { -354, -182,   99,  320,  228,  365,  228,  -81,  166, -182, -364, -213, -364, -182,  198,  363,},
    {  349,  166, -116, -329, -329, -329,  -81,  228,  133,  358,  255,  -81,  133, -241, -354,  -81,},
    { -343, -150,  133,  336,  365,  228,  -81, -329, -343, -291,   27,  320,  280,  336,   27, -311,},
    {  258,  133, -267, -329,  258,   99, -311, -267,  258,   63, -343, -182,  258,   27, -361,  -81,},
    {   27,  349,  228, -182,  166,  358,   27, -343,  280,  280, -182, -343,  349,  133, -329, -182,},
    { -361,  -81,  301,  301, -213,  228,  336,  -45,   63,  365,   63, -343,  301,  228, -267, -267,},
    {  -81, -361, -182,  228, -361, -150,  280,  301, -182,  280,  280, -182,  228,  301, -182, -329,},
    {  349,   27, -329, -267, -116, -364,  -81,  320, -343,   63,  365,   63,  133,  349,  -81, -361,},
    {  133,  365,  133, -267,  255, -182, -354,   -9, -343, -182,  280,  280,   27,  365,   27, -361,},
    { -329,   27,  349,  228,  349,  198, -241, -329, -182, -343,   63,  365,  -81,  349,  133, -329,},
    { -182, -361,  -81,  301,   63,  363,  133, -291,   63, -343, -182,  280, -182,  301,  228, -267,},
    {  258,   -9, -364,   27,  258,  -45, -354,  133,  258,  -81, -329,  228,  258, -116, -291,  301,},
    {  363,  -45, -361,   63,  320, -213, -267,  280,  228, -329,  -81,  365,   99, -364,  133,  280,},
    {  358,  -81, -354,   99,  198, -329, -116,  358,  -81, -329,  228,  228, -311,  -81,  363, -150,},
    {  349, -116, -343,  133,   27, -364,   63,  349, -329,  -81,  365,  -81, -267,  320,   63, -361,},
    {  336, -150, -329,  166, -150, -311,  228,  255, -329,  228,  228, -329,  166,  255, -329,  -45,},
    {  320, -182, -311,  198, -291, -182,  336,   99,  -81,  365,  -81, -329,  358, -182, -241,  336,},
    {  301, -213, -291,  228, -361,   -9,  363,  -81,  228,  228, -329,  -81,   27, -354,  198,  228,},
    {  280, -241, -267,  255, -343,  166,  301, -241,  365,  -81, -329,  228, -343,   -9,  349, -213,},
    {  258, -150, -241,  349,  258, -182, -182,  365,  258, -213, -116,  349,  258, -241,  -45,  301,},
    {  -45, -311,  301,   63, -182, -182,  365, -182, -291,   -9,  301, -343, -354,  166,  133, -343,},
    { -354,  228,  166, -364, -182,  365, -182, -182,   99,  228, -364,  198,  320,  -81, -213,  363,},
    {  133,  255, -343,   27,  365, -182, -182,  365,  133, -354,  280,   27, -267,   -9,  280, -361,},
    {  320, -291,  -81,  358, -182, -182,  365, -182, -311,  336,  -81, -241,  198,   99, -329,  336,},
    { -213, -182,  363, -116, -182,  365, -182, -182,  363, -182, -150,  358, -116, -182,  358, -291,},
    { -267,  336,   -9, -329,  365, -182, -182,  365, -267,  -45,  320, -329,   27,  255, -364,  228,},
    {  280,   99, -361,  198, -182, -182,  365, -182,   63,  255, -361,  166,   63, -311,  349, -150,},
    {  258, -267,   27,  228,  258, -291,   99,  133,  258, -311,  166,   27,  258, -329,  228,  -81,},
    { -361,  301,  -81, -182, -311,  363, -267,   63, -213,  336, -361,  280,  -81,  228, -329,  365,},
    {  349, -329,  133,  133,  166, -329,  358, -241, -116,  -81,  255, -354, -329,  228,  -81,  -81,},
    { -329,  349, -182,  -81,   27,  198, -343,  349,  349, -241,   63,  133,  228, -329,  365, -329,},
    {  301, -361,  228,   27, -213,   -9,  228, -354, -291,  363, -329,  198,  228,  -81,  -81,  228,},
    { -267,  365, -267,   27,  336, -182,  -45,  255,   -9, -182,  320, -364, -329,  365, -329,  228,},
    {  228, -361,  301,  -81, -361,  320, -150,  -81,  301, -150,  -45,  228,  -81,  -81,  228, -329,},
    { -182,  349, -329,  133,  280, -364,  301, -116, -343,  358, -267,   99,  365, -329,  228,  -81,},
    {  258, -343,  280, -182,  258, -354,  320, -267,  258, -361,  349, -329,  258, -364,  363, -361,},
    {   63,   63, -182,  280,  198, -116,   27,   63,  301, -267,  228, -182,  358, -354,  349, -343,},
    { -343,  365, -343,  280, -150,  228, -291,  336,  133,  -81,   27,   27,  336, -329,  320, -311,},
    { -182,   63,   63, -182, -361,  363, -343,  301,  -81,  133, -182,  228,  301, -291,  280, -267,},
    {  280, -343,  365, -343, -241,  166,  -81,   -9, -267,  301, -329,  349,  255, -241,  228, -213,},
    {  280, -182,   63,   63,   99, -182,  255, -311, -361,  365, -361,  349,  198, -182,  166, -150,},
    { -182,  280, -343,  365,  349, -364,  358, -329, -329,  301, -267,  228,  133, -116,   99,  -81,},
    { -343,  280, -182,   63,  280, -213,  133,  -45, -182,  133,  -81,   27,   63,  -45,   27,   -9,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DCT_VIII_32x32_per_CG[64][16]) ={
    {  359,  358,  357,  354,  358,  351,  336,  314,  357,  336,  296,  238,  354,  314,  238,  135,},
    {  351,  347,  342,  336,  285,  251,  211,  167,  167,   86,    0,  -86,   17, -103, -211, -296,},
    {  329,  322,  314,  305,  119,   69,   17,  -35, -167, -238, -296, -336, -347, -358, -329, -263,},
    {  296,  285,  275,  263,  -86, -135, -182, -225, -357, -357, -336, -296, -167,  -52,   69,  182,},
    {  251,  238,  225,  211, -263, -296, -322, -342, -238, -167,  -86,    0,  275,  336,  359,  342,},
    {  197,  182,  167,  151, -354, -359, -357, -347,   86,  167,  238,  296,  285,  197,   86,  -35,},
    {  135,  119,  103,   86, -329, -305, -275, -238,  336,  357,  357,  336, -151, -251, -322, -357,},
    {   69,   52,   35,   17, -197, -151, -103,  -52,  296,  238,  167,   86, -351, -305, -225, -119,},
    {  351,  285,  167,   17,  347,  251,   86, -103,  342,  211,    0, -211,  336,  167,  -86, -296,},
    { -135, -263, -342, -357, -263, -351, -342, -238, -342, -342, -211,    0, -357, -238,    0,  238,},
    { -305, -197,  -52,  103,  -69,  119,  275,  354,  211,  342,  342,  211,  357,  296,   86, -167,},
    {  238,  329,  359,  322,  336,  225,   52, -135,    0, -211, -342, -342, -336, -336, -167,   86,},
    {  225,   86,  -69, -211, -285, -357, -329, -211, -211,    0,  211,  342,  296,  357,  238,    0,},
    { -314, -358, -336, -251,  -35,  151,  296,  358,  342,  211,    0, -211, -238, -357, -296,  -86,},
    { -119,   35,  182,  296,  322,  197,   17, -167, -342, -342, -211,    0,  167,  336,  336,  167,},
    {  354,  347,  275,  151, -305, -359, -314, -182,  211,  342,  342,  211,  -86, -296, -357, -238,},
    {  329,  119, -167, -347,  322,   69, -238, -358,  314,   17, -296, -329,  305,  -35, -336, -263,},
    { -305,  -69,  211,  357, -197,  119,  342,  296,  -52,  275,  342,   86,  103,  354,  211, -167,},
    {  275,   17, -251, -359,   17, -275, -351, -151, -251, -351, -119,  225, -359, -151,  225,  351,},
    { -238,   35,  285,  354,  167,  354,  263,  -35,  357,  151, -197, -359,   86, -275, -329,  -17,},
    {  197,  -86, -314, -342, -305, -336, -103,  211, -182,  167,  358,  211,  314,  296,  -52, -342,},
    { -151,  135,  336,  322,  359,  225,  -86, -329, -135, -354, -238,  103, -251,  119,  357,  197,},
    {  103, -182, -351, -296, -314,  -52,  251,  357,  347,  263,  -69, -336, -182, -358, -135,  238,},
    {  -52,  225,  358,  263,  182, -135, -347, -285, -285,   35,  322,  305,  347,   69, -285, -322,},
    {  296,  -86, -357, -167,  285, -135, -357,  -52,  275, -182, -336,   69,  263, -225, -296,  182,},
    {  238,  336,    0, -336,  329,  225, -211, -336,  359,   52, -342, -167,  322, -135, -342,   86,},
    { -238,  167,  357,   86,   35,  354,  151, -275,  285,  263, -197, -329,  354,  -35, -359,  -17,},
    { -296, -296,   86,  357, -296,  119,  358,   69,   86,  358,   35, -347,  357,   69, -347, -119,},
    {  167, -238, -336,    0, -322, -238,  197,  342, -151,  296,  251, -211,  329,  167, -305, -211,},
    {  336,  238, -167, -357,  -17, -351, -167,  263, -322,  103,  357,   17,  275,  251, -238, -285,},
    {  -86,  296,  296,  -86,  305, -103, -359,  -86, -351, -135,  305,  238,  197,  314, -151, -336,},
    { -357, -167,  238,  336,  314,  251, -182, -347, -225, -314,  119,  354,  103,  351,  -52, -358,},
    {  251, -263, -238,  275,  238, -296, -167,  336,  225, -322,  -86,  359,  211, -342,    0,  342,},
    {  225, -285, -211,  296,   86, -357,    0,  357,  -69, -329,  211,  238, -211, -211,  342,    0,},
    {  197, -305, -182,  314,  -86, -336,  167,  296, -314, -103,  358,  -52, -342,  211,  211, -342,},
    {  167, -322, -151,  329, -238, -238,  296,  167, -336,  197,  251, -305,    0,  342, -211, -211,},
    {  135, -336, -119,  342, -336,  -86,  357,    0, -119,  357,  -35, -342,  342,    0, -342,  211,},
    {  103, -347,  -86,  351, -357,   86,  336, -167,  182,  263, -296, -135,  211, -342,    0,  342,},
    {   69, -354,  -52,  357, -296,  238,  238, -296,  354,  -17, -347,  167, -211, -211,  342,    0,},
    {   35, -358,  -17,  359, -167,  336,   86, -357,  275, -285, -151,  351, -342,  211,  211, -342,},
    {  197, -354,   86,  285,  182, -359,  167,  197,  167, -357,  238,   86,  151, -347,  296,  -35,},
    { -314,  -35,  342, -238, -358,  151,  211, -357, -336,  296,    0, -296, -251,  358, -211,  -86,},
    { -151,  359, -135, -251,  135,  225, -354,  119,  336,  -86, -238,  357,  322, -329,  103,  197,},
    {  336,  -17, -322,  275,  238, -351,  103,  251, -167, -167,  357, -238, -357,  263,   17, -285,},
    {  103, -357,  182,  211, -347,   86,  263, -342,  -86,  336, -296,    0,  351, -167, -135,  342,},
    { -351,   69,  296, -305,   69,  275, -336,   52,  296, -336,   86,  238, -305,   52,  238, -359,},
    {  -52,  347, -225, -167,  285, -329,   35,  296, -357,  167,  167, -357,  225,   69, -314,  336,},
    {  358, -119, -263,  329, -322,   17,  305, -314,  238,   86, -336,  296, -119, -182,  354, -275,},
    {  135, -329,  336, -151,  119, -305,  357, -251,  103, -275,  357, -322,   86, -238,  336, -357,},
    { -119,  322, -342,  167,   35,  197, -342,  336,  182,   17, -211,  336,  296, -167,    0,  167,},
    {  103, -314,  347, -182, -182,  -52,  263, -358, -351,  251,  -69, -135, -296,  357, -336,  238,},
    {  -86,  305, -351,  197,  296, -103, -135,  314,  296, -359,  305, -151,  -86,  -86,  238, -336,},
    {   69, -296,  354, -211, -354,  238,  -17, -211,  -52,  238, -347,  342,  357, -296,  167,    0,},
    {  -52,  285, -357,  225,  347, -329,  167,   69, -225,   35,  167, -314, -167,  296, -357,  336,},
    {   35, -275,  358, -238, -275,  359, -285,   86,  358, -285,  119,   86, -238,   86,   86, -238,},
    {  -17,  263, -359,  251,  151, -322,  351, -225, -263,  354, -329,  197,  336, -357,  296, -167,},
    {   69, -197,  296, -351,   52, -151,  238, -305,   35, -103,  167, -225,   17,  -52,   86, -119,},
    {  354, -305,  211,  -86,  347, -359,  342, -296,  275, -314,  342, -357,  151, -182,  211, -238,},
    {  -52,  182, -285,  347,  225, -135,   35,   69,  358, -347,  322, -285,  263, -285,  305, -322,},
    { -357,  314, -225,  103, -167,  251, -314,  351,  238, -182,  119,  -52,  336, -347,  354, -358,},
    {   35, -167,  275, -342, -358,  336, -285,  211,  -17,   86, -151,  211,  359, -357,  351, -342,},
    {  358, -322,  238, -119, -119,   17,   86, -182, -263,  305, -336,  354,  329, -314,  296, -275,},
    {  -17,  151, -263,  336,  263, -322,  354, -357, -359,  351, -329,  296,  251, -225,  197, -167,},
    { -359,  329, -251,  135,  329, -275,  197, -103, -251,  197, -135,   69,  135, -103,   69,  -35,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DST_I_32x32_per_CG[64][16]) ={
    {   34,   67,  100,  133,   67,  133,  193,  246,  100,  193,  269,  324,  133,  246,  324,  356,},
    {  163,  193,  220,  246,  290,  324,  346,  356,  353,  353,  324,  269,  337,  269,  163,   34,},
    {  269,  290,  309,  324,  353,  337,  309,  269,  193,  100,    0, -100, -100, -220, -309, -353,},
    {  337,  346,  353,  356,  220,  163,  100,   34, -193, -269, -324, -353, -346, -290, -193,  -67,},
    {  356,  353,  346,  337,  -34, -100, -163, -220, -353, -324, -269, -193,   67,  193,  290,  346,},
    {  324,  309,  290,  269, -269, -309, -337, -353, -100,    0,  100,  193,  353,  309,  220,  100,},
    {  246,  220,  193,  163, -356, -346, -324, -290,  269,  324,  353,  353,  -34, -163, -269, -337,},
    {  133,  100,   67,   34, -246, -193, -133,  -67,  324,  269,  193,  100, -356, -324, -246, -133,},
    {  163,  290,  353,  337,  193,  324,  353,  269,  220,  346,  324,  163,  246,  356,  269,   34,},
    {  246,  100,  -67, -220,  100, -100, -269, -353,  -67, -269, -356, -290, -220, -353, -290,  -67,},
    { -324, -356, -309, -193, -324, -193,    0,  193, -100,  133,  309,  353,  193,  346,  309,  100,},
    {  -34,  133,  269,  346,  324,  353,  269,  100,  246,   34, -193, -337, -163, -337, -324, -133,},
    {  346,  269,  133,  -34, -100, -269, -353, -324, -337, -193,   34,  246,  133,  324,  337,  163,},
    { -193, -309, -356, -324, -193,    0,  193,  324,  353,  309,  133, -100, -100, -309, -346, -193,},
    { -220,  -67,  100,  246,  353,  269,  100, -100, -290, -356, -269,  -67,   67,  290,  353,  220,},
    {  337,  353,  290,  163, -269, -353, -324, -193,  163,  324,  346,  220,  -34, -269, -356, -246,},
    {  269,  353,  193, -100,  290,  337,  100, -220,  309,  309,    0, -309,  324,  269, -100, -353,},
    { -324, -324, -100,  193, -356, -193,  133,  346, -309,    0,  309,  309, -193,  193,  353,  100,},
    {  353,  269,    0, -269,  269,  -34, -309, -324,    0, -309, -309,    0, -269, -324,    0,  324,},
    { -353, -193,  100,  324,  -67,  246,  353,  163,  309,  309,    0, -309,  269, -100, -353, -193,},
    {  324,  100, -193, -353, -163, -353, -246,   67, -309,    0,  309,  309,  193,  353,  100, -269,},
    { -269,    0,  269,  353,  324,  309,   34, -269,    0, -309, -309,    0, -324,    0,  324,  269,},
    {  193, -100, -324, -324, -346, -133,  193,  356,  309,  309,    0, -309, -100, -353, -193,  193,},
    { -100,  193,  353,  269,  220, -100, -337, -290, -309,    0,  309,  309,  353,  100, -269, -324,},
    {  337,  220, -193, -346,  346,  163, -269, -290,  353,  100, -324, -193,  356,   34, -353,  -67,},
    {  -34,  324,  246, -163,  133,  353,   34, -337,  269,  269, -193, -324,  346,  100, -337, -133,},
    { -353,  -67,  309,  269, -193,  246,  309, -100,  100,  353,    0, -353,  324,  163, -309, -193,},
    { -133, -356, -100,  290, -356,  -67,  324,  220, -100,  324,  193, -269,  290,  220, -269, -246,},
    {  290, -100, -356, -133, -220, -324,   67,  356, -269,  193,  324, -100,  246,  269, -220, -290,},
    {  269,  309,  -67, -353,  100, -309, -246,  193, -353,    0,  353,  100,  193,  309, -163, -324,},
    { -163,  246,  324,  -34,  337,  -34, -353, -133, -324, -193,  269,  269,  133,  337, -100, -346,},
    { -346, -193,  220,  337,  290,  269, -163, -346, -193, -324,  100,  353,   67,  353,  -34, -356,},
    {  356,  -34, -353,   67,  353, -100, -324,  193,  346, -163, -269,  290,  337, -220, -193,  346,},
    {  346, -100, -337,  133,  269, -269, -193,  324,  133, -353,   34,  337,  -34, -324,  246,  163,},
    {  324, -163, -309,  193,  100, -353,    0,  353, -193, -246,  309,  100, -353,   67,  309, -269,},
    {  290, -220, -269,  246, -100, -324,  193,  269, -356,   67,  324, -220, -133,  356, -100, -290,},
    {  246, -269, -220,  290, -269, -193,  324,  100, -220,  324,   67, -356,  290,  100, -356,  133,},
    {  193, -309, -163,  324, -353,    0,  353, -100,  100,  309, -246, -193,  269, -309,  -67,  353,},
    {  133, -337, -100,  346, -324,  193,  269, -269,  337,   34, -353,  133, -163, -246,  324,   34,},
    {   67, -353,  -34,  356, -193,  324,  100, -353,  290, -269, -163,  346, -346,  193,  220, -337,},
    {  324, -269, -100,  353,  309, -309,    0,  309,  290, -337,  100,  220,  269, -353,  193,  100,},
    { -193, -193,  353, -100, -309,    0,  309, -309, -356,  193,  133, -346, -324,  324, -100, -193,},
    { -269,  324,    0, -324,    0,  309, -309,    0,  269,   34, -309,  324,  353, -269,    0,  269,},
    {  269,  100, -353,  193,  309, -309,    0,  309,  -67, -246,  353, -163, -353,  193,  100, -324,},
    {  193, -353,  100,  269, -309,    0,  309, -309, -163,  353, -246,  -67,  324, -100, -193,  353,},
    { -324,    0,  324, -269,    0,  309, -309,    0,  324, -309,   34,  269, -269,    0,  269, -353,},
    { -100,  353, -193, -193,  309, -309,    0,  309, -346,  133,  193, -356,  193,  100, -324,  324,},
    {  353, -100, -269,  324, -309,    0,  309, -309,  220,  100, -337,  290, -100, -193,  353, -269,},
    {  246, -356,  269,  -34,  220, -346,  324, -163,  193, -324,  353, -269,  163, -290,  353, -337,},
    { -220,  353, -290,   67,  -67,  269, -356,  290,  100,  100, -269,  353,  246, -100,  -67,  220,},
    {  193, -346,  309, -100, -100, -133,  309, -353, -324,  193,    0, -193, -324,  356, -309,  193,},
    { -163,  337, -324,  133,  246,  -34, -193,  337,  324, -353,  269, -100,  -34, -133,  269, -346,},
    {  133, -324,  337, -163, -337,  193,   34, -246, -100,  269, -353,  324,  346, -269,  133,   34,},
    { -100,  309, -346,  193,  353, -309,  133,  100, -193,    0,  193, -324, -193,  309, -356,  324,},
    {   67, -290,  353, -220, -290,  356, -269,   67,  353, -269,  100,  100, -220,   67,  100, -246,},
    {  -34,  269, -356,  246,  163, -324,  346, -220, -269,  353, -324,  193,  337, -353,  290, -163,},
    {  133, -246,  324, -356,  100, -193,  269, -324,   67, -133,  193, -246,   34,  -67,  100, -133,},
    {  337, -269,  163,  -34,  353, -353,  324, -269,  290, -324,  346, -356,  163, -193,  220, -246,},
    { -100,  220, -309,  353,  193, -100,    0,  100,  353, -337,  309, -269,  269, -290,  309, -324,},
    { -346,  290, -193,   67, -193,  269, -324,  353,  220, -163,  100,  -34,  337, -346,  353, -356,},
    {   67, -193,  290, -346, -353,  324, -269,  193,  -34,  100, -163,  220,  356, -353,  346, -337,},
    {  353, -309,  220, -100, -100,    0,  100, -193, -269,  309, -337,  353,  324, -309,  290, -269,},
    {  -34,  163, -269,  337,  269, -324,  353, -353, -356,  346, -324,  290,  246, -220,  193, -163,},
    { -356,  324, -246,  133,  324, -269,  193, -100, -246,  193, -133,   67,  133, -100,   67,  -34,},
};

DECLARE_ALIGNED(16, static const int16_t, TR_DST_VII_32x32_per_CG[64][16]) ={
    {   17,   52,   86,  119,   35,  103,  167,  225,   52,  151,  238,  305,   69,  197,  296,  351,},
    {  151,  182,  211,  238,  275,  314,  342,  357,  347,  359,  342,  296,  354,  305,  211,   86,},
    {  263,  285,  305,  322,  358,  347,  322,  285,  225,  135,   35,  -69,  -52, -182, -285, -347,},
    {  336,  347,  354,  358,  238,  182,  119,   52, -167, -251, -314, -351, -357, -314, -225, -103,},
    {  359,  357,  351,  342,  -17,  -86, -151, -211, -358, -336, -285, -211,   35,  167,  275,  342,},
    {  329,  314,  296,  275, -263, -305, -336, -354, -119,  -17,   86,  182,  358,  322,  238,  119,},
    {  251,  225,  197,  167, -359, -351, -329, -296,  263,  322,  354,  357,  -17, -151, -263, -336,},
    {  135,  103,   69,   35, -251, -197, -135,  -69,  329,  275,  197,  103, -359, -329, -251, -135,},
    {   86,  238,  336,  357,  103,  275,  357,  322,  119,  305,  357,  251,  135,  329,  336,  151,},
    {  296,  167,    0, -167,  182,  -17, -211, -336,   35, -197, -342, -336, -119, -322, -342, -167,},
    { -296, -357, -336, -238, -351, -251,  -69,  135, -182,   52,  263,  358,  103,  314,  347,  182,},
    {  -86,   86,  238,  336,  296,  359,  305,  151,  296,  103, -135, -314,  -86, -305, -351, -197,},
    {  357,  296,  167,    0,  -52, -238, -347, -342, -354, -238,  -17,  211,   69,  296,  354,  211,},
    { -167, -296, -357, -336, -225,  -35,  167,  314,  347,  329,  167,  -69,  -52, -285, -357, -225,},
    { -238,  -86,   86,  238,  358,  285,  119,  -86, -275, -359, -285,  -86,   35,  275,  358,  238,},
    {  336,  357,  296,  167, -263, -354, -329, -197,  151,  322,  351,  225,  -17, -263, -359, -251,},
    {  151,  347,  296,   35,  167,  357,  238,  -86,  182,  359,  167, -197,  197,  354,   86, -285,},
    { -251, -358, -211,   86, -336, -296,    0,  296, -358, -151,  211,  357, -314,   35,  342,  238,},
    {  322,  329,  103, -197,  336,   86, -238, -357,  135, -225, -354, -119, -151, -359, -135,  251,},
    { -357, -263,   17,  285, -167,  167,  357,  238,  238,  351,  103, -251,  336,   17, -322, -275,},
    {  351,  167, -135, -342,  -86, -336, -296,    0, -347,  -86,  263,  342,  103,  357,  182, -211,},
    { -305,  -52,  238,  359,  296,  336,   86, -238,   69, -275, -336,  -52, -351,  -69,  296,  305,},
    {  225,  -69, -314, -336, -357, -167,  167,  357,  285,  329,   35, -296,  -52, -347, -225,  167,},
    { -119,  182,  354,  275,  238,  -86, -336, -296, -322,  -17,  305,  314,  358,  119, -263, -329,},
    {  211,  342,    0, -342,  225,  322,  -86, -359,  238,  296, -167, -336,  251,  263, -238, -275,},
    { -211,  211,  342,    0,  -69,  329,  211, -238,   86,  357,    0, -357,  225,  285, -211, -296,},
    { -342, -211,  211,  342, -314,  103,  358,   52,  -86,  336,  167, -296,  197,  305, -182, -314,},
    {    0, -342, -211,  211, -336, -197,  251,  305, -238,  238,  296, -167,  167,  322, -151, -329,},
    {  342,    0, -342, -211, -119, -357,  -35,  342, -336,   86,  357,    0,  135,  336, -119, -342,},
    {  211,  342,    0, -342,  182, -263, -296,  135, -357,  -86,  336,  167,  103,  347,  -86, -351,},
    { -211,  211,  342,    0,  354,   17, -347, -167, -296, -238,  238,  296,   69,  354,  -52, -357,},
    { -342, -211,  211,  342,  275,  285, -151, -351, -167, -336,   86,  357,   35,  358,  -17, -359,},
    {  263,  225, -296, -182,  275,  182, -336,  -69,  285,  135, -357,   52,  296,   86, -357,  167,},
    {  322,  135, -342,  -86,  359,  -52, -342,  167,  329, -225, -211,  336,  238, -336,    0,  336,},
    {  354,   35, -359,   17,  285, -263, -197,  329,   35, -354,  151,  275, -238, -167,  357,  -86,},
    {  357,  -69, -347,  119,   86, -358,   35,  347, -296, -119,  358,  -69, -296,  296,   86, -357,},
    {  329, -167, -305,  211, -151, -296,  251,  211, -322,  238,  197, -342,  167,  238, -336,    0,},
    {  275, -251, -238,  285, -322, -103,  357,  -17,  -17,  351, -167, -263,  336, -238, -167,  357,},
    {  197, -314, -151,  336, -351,  135,  305, -238,  305,  103, -359,   86,  -86, -296,  296,   86,},
    {  103, -351,  -52,  358, -225,  314,  119, -354,  314, -251, -182,  347, -357,  167,  238, -336,},
    {  305,   35, -336,  263,  314,  -17, -296,  329,  322,  -69, -238,  358,  329, -119, -167,  347,},
    {  103, -354,  211,  167,  -52, -275,  342,  -86, -197, -119,  342, -296, -305,   69,  211, -357,},
    { -359,  151,  225, -351, -251,  351, -119, -225,   17,  275, -351,  151,  275,  -17, -251,  359,},
    {   86,  275, -329,   17,  357, -151, -197,  359,  167, -354,  263,   35, -238,  -35,  285, -354,},
    {  314, -296,  -52,  342, -182, -167,  358, -211, -305,  336, -103, -211,  197,   86, -314,  342,},
    { -251, -119,  357, -197, -135,  354, -238, -103,  359, -225,  -86,  329, -151, -135,  336, -322,},
    { -182,  358, -135, -238,  347, -263,  -69,  336, -314,   52,  251, -357,  103,  182, -351,  296,},
    {  347,  -69, -285,  322, -285,  -35,  322, -305,  182,  135, -347,  285,  -52, -225,  358, -263,},
    {  336, -167,  -86,  296,  342, -211,    0,  211,  347, -251,   86,  103,  351, -285,  167,  -17,},
    { -357,  238,    0, -238, -342,  342, -211,    0, -263,  351, -342,  238, -135,  263, -342,  357,},
    {  357, -296,   86,  167,  211, -342,  342, -211,  -69, -119,  275, -354, -305,  197,  -52, -103,},
    { -336,  336, -167,  -86,    0,  211, -342,  342,  336, -225,   52,  135,  238, -329,  359, -322,},
    {  296, -357,  238,    0, -211,    0,  211, -342, -285,  357, -329,  211,  225,  -86,  -69,  211,},
    { -238,  357, -296,   86,  342, -211,    0,  211,  -35, -151,  296, -358, -314,  358, -336,  251,},
    {  167, -336,  336, -167, -342,  342, -211,    0,  322, -197,   17,  167, -119,  -35,  182, -296,},
    {  -86,  296, -357,  238,  211, -342,  342, -211, -305,  359, -314,  182,  354, -347,  275, -151,},
    {  354, -314,  238, -135,  357, -336,  296, -238,  358, -351,  336, -314,  359, -358,  357, -354,},
    {   17,  103, -211,  296,  167,  -86,    0,   86,  285, -251,  211, -167,  351, -347,  342, -336,},
    { -347,  358, -329,  263, -167,  238, -296,  336,  119,  -69,   17,   35,  329, -322,  314, -305,},
    { -167,   52,   69, -182, -357,  357, -336,  296,  -86,  135, -182,  225,  296, -285,  275, -263,},
    {  275, -336,  359, -342, -238,  167,  -86,    0, -263,  296, -322,  342,  251, -238,  225, -211,},
    {  285, -197,   86,   35,   86, -167,  238, -296, -354,  359, -357,  347,  197, -182,  167, -151,},
    { -151,  251, -322,  357,  336, -357,  357, -336, -329,  305, -275,  238,  135, -119,  103,  -86,},
    { -351,  305, -225,  119,  296, -238,  167,  -86, -197,  151, -103,   52,   69,  -52,   35,  -17,},
};



#endif
#endif

#define SHIFT_EMT_V (EMT_TRANSFORM_MATRIX_SHIFT + 1 + COM16_C806_TRANS_PREC)
#define ADD_EMT_V (1 << (SHIFT_EMT_V - 1))

#define VAR_DECL_AND_LOAD4X4_AVX2_V()\
    const __m256i add  = _mm256_set1_epi32(ADD_EMT_V);\
    __m256i x0,x4,x8,x12;                             \
    __m256i _src = _mm256_load_si256((__m256i *)src); \

#define VAR_DECL_AND_LOAD4X4_AVX2_H()\
    const int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;\
    const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));\
    __m256i x0,x4,x8,x12;                                    \
    __m256i _src = _mm256_load_si256((__m256i *)src);        \

#define TRANSPOSE_4X4_AVX2()\
    x0   = _mm256_permute4x64_epi64(_src, 0b00111100);         \
    x4   = _mm256_permute4x64_epi64(_src, 0b10010110);         \
    x8   = _mm256_unpacklo_epi16(x0, x4);                      \
    x12  = _mm256_unpackhi_epi16(x4,x0);                       \
    _src = _mm256_unpacklo_epi16(x8,x12);                      \
    x12  = _mm256_unpackhi_epi16(x8,x12);                      \
    x0   = _mm256_set1_epi64x(_mm256_extract_epi64(_src,0b00));\
    x4   = _mm256_set1_epi64x(_mm256_extract_epi64(_src,0b01));\
    x8   = _mm256_set1_epi64x(_mm256_extract_epi64(x12,0b00)); \
    x12  = _mm256_set1_epi64x(_mm256_extract_epi64(x12,0b01)); \

#define COMPUTE4X4_AVX2()\
    x0  = _mm256_madd_epi16(x0 , dct_matrix);      \
    x4  = _mm256_madd_epi16(x4 , dct_matrix);      \
    x8  = _mm256_madd_epi16(x8 , dct_matrix);      \
    x12 = _mm256_madd_epi16(x12, dct_matrix);      \
    x0  = _mm256_hadd_epi32(x0, x4);               \
    x0  = _mm256_permute4x64_epi64(x0, 0b11011000);\
    x8  = _mm256_hadd_epi32(x8, x12);              \
    x8  = _mm256_permute4x64_epi64(x8, 0b11011000);\

#define ROUND_4X4_AVX2_V()\
    x0 =  _mm256_add_epi32(x0,add);          \
    x8 =  _mm256_add_epi32(x8,add);          \
    x0 =  _mm256_srai_epi32(x0,SHIFT_EMT_V); \
    x8 =  _mm256_srai_epi32(x8,SHIFT_EMT_V); \

#define ROUND_4X4_AVX2_H()\
    x0 =  _mm256_add_epi32(x0,add);    \
    x8 =  _mm256_add_epi32(x8,add);    \
    x0 =  _mm256_srai_epi32(x0,shift); \
    x8 =  _mm256_srai_epi32(x8,shift); \

#define PACK_CLIP_AND_STORE_4X4_AVX2()            \
    x0 = _mm256_packs_epi32(x0,x8);               \
    x0 = _mm256_permute4x64_epi64(x0, 0b11011000);\
    _mm256_store_si256((__m256i *)&dst[0],x0);    \

#define debug4X4_16(x) \
    _mm256_store_si256((__m256i *)&dst[0],x);\
    fprintf(stderr,"temp %s\n",#x);         \
    for (int j=0; j < 4; j++){\
    for (int i=0;i<4;i++){                    \
    fprintf(stderr,"%d ",dst[j*4+i]);     \
    }                                         \
    fprintf(stderr,"\n");                     \
    }\

#define debug_32(x) \
    _mm256_store_si256((__m256i *)&dst[0],x);\
    fprintf(stderr,"temp %s\n",#x);         \
    for (int i=0;i<8;i++){                    \
    fprintf(stderr,"%d ",*((int32_t *)&dst[2*i]));     \
    }\
    fprintf(stderr,"\n");

//extern __inline__ uint64_t rdtsc(void) {
//  uint64_t x;
//  x = clock_gettime(CLOCK_MONOTONIC)
//  //__asm__ volatile ("rdtsc" : "=A" (x));
//  return x;
//}


//void FUNC(emt_idst_VII_4x4_v_avx2) (int16_t *x, int16_t *block, int log2_transform_range, const int clip_min, const int clip_max)
//{
//    //const __m128i zeros = _mm_setzero_si128();
//    //const __m128i add  = _mm_set1_epi32(ADD_EMT_V);

//    //const __m128i max  = _mm_set1_epi16(clip_max);
//    //const __m128i min  = _mm_set1_epi16(clip_min);
//    //fprintf(stderr,"AMT\n");

////    const __m128i a_VII = _mm_set1_epi16(-336);
////    const __m128i b_VII = _mm_set1_epi16(296);
////    const __m128i c_VII = _mm_set1_epi16(219);
////    const __m128i d_VII = _mm_set1_epi16(117);
////     struct timespec t1,t2;

////     clock_gettime(CLOCK_REALTIME,&t1);
//    __m128i c0, c1,c2,c3,c4, x0, x8, x12, x4;

//      x0  = _mm_load_si128((__m128i *)x);
//      x8  = _mm_loadl_epi64((__m128i *)&x[8]);
//      x12 = _mm_loadl_epi64((__m128i *)&x[12]);
//      //x4  = _mm_loadl_epi64((__m128i *)&x[4]);
//      //x12 = _mm_shuffle_epi32(x8,0b01001110);

//      c0 = _mm_unpacklo_epi16(x8,x12);
//      c2 = _mm_unpacklo_epi16(x0,x8);

//      //c3 = _mm_sub_epi16(_mm_setzero_si128(),x12);
//      c3 = _mm_unpacklo_epi16(x0,x12);

//      c4 = _mm_unpackhi_epi16(x0,_mm_setzero_si128());

//      c1 = _mm_sub_epi16(x0,x8);
//      c1 = _mm_unpacklo_epi16(c1,x12);

//      c0 = _mm_madd_epi16(c0,_mm_set1_epi16(-336));
//      c1 = _mm_madd_epi16(c1,_mm_set1_epi16(296));
//      c2 = _mm_madd_epi16(c2,_mm_set1_epi16(219));
//      c3 = _mm_madd_epi16(c3,_mm_set_epi16(-117,117,-117,117,-117,117,-117,117));

//      c4 = _mm_madd_epi16(c4,_mm_set1_epi16(296));

//      x0 = _mm_sub_epi32(c3,c0);
//      x0 = _mm_add_epi32(x0,c4);

//      x12 = _mm_add_epi32(c2,c3);
//      x12 = _mm_sub_epi32(x12,c4);

//      x4 = _mm_add_epi32(c2,c0);
//      x4 = _mm_add_epi32(x4,c4);

//      x0 =  _mm_add_epi32(x0, _mm_set1_epi32(ADD_EMT_V));
//      x4 =  _mm_add_epi32(x4, _mm_set1_epi32(ADD_EMT_V));
//      x8 =  _mm_add_epi32(c1, _mm_set1_epi32(ADD_EMT_V));
//      x12 = _mm_add_epi32(x12,_mm_set1_epi32(ADD_EMT_V));

//      x0 =  _mm_srai_epi32(x0, SHIFT_EMT_V);
//      x4 =  _mm_srai_epi32(x4, SHIFT_EMT_V);
//      x8 =  _mm_srai_epi32(x8, SHIFT_EMT_V);
//      x12 = _mm_srai_epi32(x12,SHIFT_EMT_V);

////      c0 = _mm_unpacklo_epi32(x0,x8);
////      c1 = _mm_unpacklo_epi32(x4,x12);

////      c2 = _mm_unpackhi_epi32(x0,x8);
////      c3 = _mm_unpackhi_epi32(x4,x12);

////      c0 = _mm_packs_epi32(c0,c2);
////      c1 = _mm_packs_epi32(c1,c3);
//      c0 = _mm_packs_epi32(x0,x8);
//      c1 = _mm_packs_epi32(x4,x12);

//      c0 = _mm_shuffle_epi32(c0,0b11011000);
//      c1 = _mm_shuffle_epi32(c1,0b11011000);

//// We should not need clipping here since a last clip is done in h funct and 16
//// we already used 16 bits saturation
////      c0 = _mm_min_epi16(c0,max);
////      c0 = _mm_max_epi16(c0,min);

////      c1 = _mm_min_epi16(c1,max);
////      c1 = _mm_max_epi16(c1,min);

//      x0 = _mm_unpacklo_epi16(c0,c1);
//      x4 = _mm_unpackhi_epi16(c0,c1);

//      x0 = _mm_shuffle_epi32(x0,0b11011000);
//      x4 = _mm_shuffle_epi32(x4,0b11011000);

//      _mm_store_si128((__m128i *)&block[0],x0);
//      _mm_store_si128((__m128i *)&block[8],x4);

//      //uint64_t t2 = rdtsc();
////      clock_gettime(CLOCK_REALTIME,&t2);
////      fprintf(stderr,"num cycles:%ld, %ld\n",t2.tv_nsec-t1.tv_nsec,t2.tv_sec-t1.tv_sec);
//}

//void FUNC(emt_idst_VII_4x4_h_avx2)(int16_t *x, int16_t *block, int log2_transform_range, const int clip_min, const int clip_max)
//{
//  const int shift = (EMT_TRANSFORM_MATRIX_SHIFT + log2_transform_range - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;
//  const __m128i zeros = _mm_setzero_si128();
//  const __m128i add  = _mm_set1_epi32(1 << (shift - 1));

//  const __m128i max  = _mm_set1_epi16(clip_max);
//  const __m128i min  = _mm_set1_epi16(clip_min);

//  const __m128i a_VII = _mm_set1_epi16(-336);
//  const __m128i b_VII = _mm_set1_epi16(296);
//  const __m128i c_VII = _mm_set1_epi16(219);
//  const __m128i d_VII = _mm_set_epi16(-117,117,-117,117,-117,117,-117,117);

//  __m128i c0, c1,c2,c3,c4, x0, x8, x12, x4;
//  //fprintf(stderr,"AMT\n");

//    x0  = _mm_load_si128((__m128i *)x);
//    x8  = _mm_loadl_epi64((__m128i *)&x[8]);
//    x12 = _mm_loadl_epi64((__m128i *)&x[12]);
//    //x4  = _mm_loadl_epi64((__m128i *)&x[4]);
//    //x12 = _mm_shuffle_epi32(x8,0b01001110);

//    c0 = _mm_unpacklo_epi16(x8,x12);
//    c2 = _mm_unpacklo_epi16(x0,x8);

//    //c3 = _mm_sub_epi16(zeros,x12);
//    c3 = _mm_unpacklo_epi16(x0,x12);

//    c4 = _mm_unpackhi_epi16(x0,zeros);

//    c1 = _mm_sub_epi16(x0,x8);
//    c1 = _mm_unpacklo_epi16(c1,x12);

//    c0 = _mm_madd_epi16(c0,a_VII);
//    c1 = _mm_madd_epi16(c1,b_VII);
//    c2 = _mm_madd_epi16(c2,c_VII);
//    c3 = _mm_madd_epi16(c3,d_VII);

//    c4 = _mm_madd_epi16(c4,b_VII);

//    x0 = _mm_sub_epi32(c3,c0);
//    x0 = _mm_add_epi32(x0,c4);

//    x12 = _mm_add_epi32(c2,c3);
//    x12 = _mm_sub_epi32(x12,c4);

//    x4 = _mm_add_epi32(c2,c0);
//    x4 = _mm_add_epi32(x4,c4);

//    x0 =  _mm_add_epi32(x0,add);
//    x4 =  _mm_add_epi32(x4,add);
//    x8 =  _mm_add_epi32(c1,add);
//    x12 = _mm_add_epi32(x12,add);

//    x0 =  _mm_srai_epi32(x0,shift);
//    x4 =  _mm_srai_epi32(x4,shift);
//    x8 =  _mm_srai_epi32(x8,shift);
//    x12 = _mm_srai_epi32(x12,shift);

//    c0 = _mm_unpacklo_epi32(x0,x8);
//    c1 = _mm_unpacklo_epi32(x4,x12);
//    c2 = _mm_unpackhi_epi32(x0,x8);
//    c3 = _mm_unpackhi_epi32(x4,x12);

//    c0 = _mm_packs_epi32(c0,c2);
//    c1 = _mm_packs_epi32(c1,c3);

//    c0 = _mm_min_epi16(c0,max);
//    c0 = _mm_max_epi16(c0,min);

//    c1 = _mm_min_epi16(c1,max);
//    c1 = _mm_max_epi16(c1,min);

//    x0 = _mm_unpacklo_epi16(c0,c1);
//    x4 = _mm_unpackhi_epi16(c0,c1);

//    _mm_store_si128((__m128i *)&block[0],x0);
//    _mm_store_si128((__m128i *)&block[8],x4);
//}

//void FUNC(emt_idct_VIII_4x4_v_avx2) (int16_t *x, int16_t *block, int log2_transform_range, const int clip_min, const int clip_max)
//{
//    const __m128i zeros = _mm_setzero_si128();
//    const __m128i add  = _mm_set1_epi32(ADD_EMT_V);

//    const __m128i max  = _mm_set1_epi16(clip_max);
//    const __m128i min  = _mm_set1_epi16(clip_min);

//    const __m128i a_VIII = _mm_set_epi16(-219,219,-219,219,-219,219,-219,219);
//    const __m128i b_VIII = _mm_set_epi16(-296,296,-296,296,-296,296,-296,296);
//    const __m128i c_VIII = _mm_set1_epi16(117);
//    const __m128i d_VIII = _mm_set1_epi16(336);

//    __m128i c0, c1,c2,c3,c4, x0, x8, x12, x4;
//    fprintf(stderr,"AMT\n");


//    x0  = _mm_load_si128((__m128i *)x);
//    x8  = _mm_loadl_epi64((__m128i *)&x[8]);
//    x12 = _mm_loadl_epi64((__m128i *)&x[12]);
//    //x4  = _mm_loadl_epi64((__m128i *)&x[4]);
//    //x12 = _mm_shuffle_epi32(x8,0b01001110);

//    //FIXME: risk of integer overflow?
//    c1 = _mm_sub_epi16(x0,x8);

//    //c0 used to store the opposit of x12 used 2x
//    //c0 = _mm_sub_epi16(zeros,x12);

//    c1 = _mm_unpacklo_epi16(c1,x12);
//    c0 = _mm_unpacklo_epi16(x8,x12);
//    c2 = _mm_unpacklo_epi16(x0,x8);
//    c3 = _mm_unpacklo_epi16(x0,x12);

//    c4 = _mm_unpackhi_epi16(x0,zeros);

//    c0 = _mm_madd_epi16(c0,a_VIII);
//    c1 = _mm_madd_epi16(c1,b_VIII);
//    c2 = _mm_madd_epi16(c2,c_VIII);
//    c3 = _mm_madd_epi16(c3,d_VIII);
//    c4 = _mm_madd_epi16(c4,b_VIII);

//    x0 = _mm_add_epi32(c3,c0);
//    x0 = _mm_add_epi32(x0,c4);

//    x8 = _mm_sub_epi32(c3,c2);
//    x8 = _mm_sub_epi32(x8,c4);

//    x12 = _mm_add_epi32(c2,c0);
//    x12 = _mm_sub_epi32(x12,c4);

//    x0 =  _mm_add_epi32(x0,add);
//    x4 =  _mm_add_epi32(c1,add);
//    x8 =  _mm_add_epi32(x8,add);
//    x12 = _mm_add_epi32(x12,add);

//    x0 =  _mm_srai_epi32(x0,SHIFT_EMT_V);
//    x4 =  _mm_srai_epi32(x4,SHIFT_EMT_V);
//    x8 =  _mm_srai_epi32(x8,SHIFT_EMT_V);
//    x12 = _mm_srai_epi32(x12,SHIFT_EMT_V);

//    c0 = _mm_unpacklo_epi32(x0,x8);
//    c1 = _mm_unpacklo_epi32(x4,x12);

//    c2 = _mm_unpackhi_epi32(x0,x8);
//    c3 = _mm_unpackhi_epi32(x4,x12);

//    c0 = _mm_packs_epi32(c0,c2);
//    c1 = _mm_packs_epi32(c1,c3);

//// We should not need to clip here since a last clip is done in h funct
////    c0 = _mm_min_epi16(c0,max);
////    c0 = _mm_max_epi16(c0,min);

////    c1 = _mm_min_epi16(c1,max);
////    c1 = _mm_max_epi16(c1,min);

//    x0 = _mm_unpacklo_epi16(c0,c1);
//    x4 = _mm_unpackhi_epi16(c0,c1);

//    _mm_store_si128((__m128i *)&block[0],x0);
//    _mm_store_si128((__m128i *)&block[8],x4);
//    fprintf(stderr,"intrinsic\n");
//    for (int i=0;i<16;i++){
//        fprintf(stderr,"%d ",block[i]);
//    }
//    fprintf(stderr,"\n");
//}



//void FUNC(emt_idct_VIII_4x4_h_avx2)(int16_t *x, int16_t *block, int log2_transform_range, const int clip_min, const int clip_max)
//{
//    int shift = (EMT_TRANSFORM_MATRIX_SHIFT + log2_transform_range - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;
//  const __m128i zeros = _mm_setzero_si128();
//  const __m128i add  = _mm_set1_epi32(1 << (shift - 1));

//  const __m128i max  = _mm_set1_epi16(clip_max);
//  const __m128i min  = _mm_set1_epi16(clip_min);

//  const __m128i a_VIII = _mm_set_epi16(-219,219,-219,219,-219,219,-219,219);
//  const __m128i b_VIII = _mm_set_epi16(-296,296,-296,296,-296,296,-296,296);
//  const __m128i c_VIII = _mm_set1_epi16(117);
//  const __m128i d_VIII = _mm_set1_epi16(336);
//  fprintf(stderr,"intrinsic\n");
//  for (int i=0;i<16;i++){
//      fprintf(stderr,"%d ",x[i]);
//  }
//  fprintf(stderr,"\n");
//  __m128i c0, c1,c2,c3,c4, x0, x8, x12, x4;

//    x0  = _mm_load_si128((__m128i *)x);
//    x8  = _mm_loadl_epi64((__m128i *)&x[8]);
//    x12 = _mm_loadl_epi64((__m128i *)&x[12]);
//    //x4  = _mm_loadl_epi64((__m128i *)&x[4]);
//    //x12 = _mm_shuffle_epi32(x8,0b01001110);

//    //FIXME: might overflow?
//    c1 = _mm_sub_epi16(x0,x8);

//    //c0 = _mm_sub_epi16(zeros,x12);

//    c1 = _mm_unpacklo_epi16(c1,x12);
//    c0 = _mm_unpacklo_epi16(x8,x12);

//    c2 = _mm_unpacklo_epi16(x0,x8);

//    c3 = _mm_unpacklo_epi16(x0,x12);

//    c4 = _mm_unpackhi_epi16(x0,zeros);

//    c0 = _mm_madd_epi16(c0,a_VIII);
//    c1 = _mm_madd_epi16(c1,b_VIII);
//    c2 = _mm_madd_epi16(c2,c_VIII);
//    c3 = _mm_madd_epi16(c3,d_VIII);

//    c4 = _mm_madd_epi16(c4,b_VIII);

//    x0 = _mm_add_epi32(c3,c0);
//    x0 = _mm_add_epi32(x0,c4);

//    x8 = _mm_sub_epi32(c3,c2);
//    x8 = _mm_sub_epi32(x8,c4);

//    x12 = _mm_add_epi32(c2,c0);
//    x12 = _mm_sub_epi32(x12,c4);

//    x0 =  _mm_add_epi32(x0,add);
//    x4 =  _mm_add_epi32(c1,add);
//    x8 =  _mm_add_epi32(x8,add);
//    x12 = _mm_add_epi32(x12,add);

//    x0 =  _mm_srai_epi32(x0,shift);
//    x4 =  _mm_srai_epi32(x4,shift);
//    x8 =  _mm_srai_epi32(x8,shift);
//    x12 = _mm_srai_epi32(x12,shift);

//    c0 = _mm_unpacklo_epi32(x0,x8);
//    c1 = _mm_unpacklo_epi32(x4,x12);
//    c2 = _mm_unpackhi_epi32(x0,x8);
//    c3 = _mm_unpackhi_epi32(x4,x12);

//    c0 = _mm_packs_epi32(c0,c2);
//    c1 = _mm_packs_epi32(c1,c3);

//    c0 = _mm_min_epi16(c0,max);
//    c0 = _mm_max_epi16(c0,min);

//    c1 = _mm_min_epi16(c1,max);
//    c1 = _mm_max_epi16(c1,min);

//    x0 = _mm_unpacklo_epi16(c0,c1);
//    x4 = _mm_unpackhi_epi16(c0,c1);

//    _mm_store_si128((__m128i *)&block[0],x0);
//    _mm_store_si128((__m128i *)&block[8],x4);


//    fprintf(stderr,"intrinsic\n");
//    for (int i=0;i<16;i++){
//        fprintf(stderr,"%d ",block[i]);
//    }
//    fprintf(stderr,"\n");
//}

#define DO_4X4_V()\
    const __m256i add  = _mm256_set1_epi32(ADD_EMT_V);                         \
    __m256i x0,x4,x8,x12,_src2;                                                \
    __m256i _src =_mm256_load_si256((__m256i *)src);                           \
    const __m256i _D0 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b01010000);\
    const __m256i _D1 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b11111010);\
    const __m256i _D2 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b01010000);\
    const __m256i _D3 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b11111010);\
    x0    = _mm256_permute4x64_epi64(_src,0b10110001);                         \
    _src  = _mm256_unpacklo_epi16(_src,x0);                                    \
    _src2 = _mm256_permute4x64_epi64(_src,0b01001110);                         \
    x0  = _mm256_madd_epi16(_src,  _D0);                                       \
    x4  = _mm256_madd_epi16(_src2, _D1);                                       \
    x8  = _mm256_madd_epi16(_src,  _D2);                                       \
    x12 = _mm256_madd_epi16(_src2, _D3);                                       \
    x0 =  _mm256_add_epi32(x0,x4);                                             \
    x8 =  _mm256_add_epi32(x8,x12);                                            \
    x0 =  _mm256_add_epi32(x0,add);                                            \
    x8 =  _mm256_add_epi32(x8,add);                                            \
    x0 =  _mm256_srai_epi32(x0,SHIFT_EMT_V);                                   \
    x8 =  _mm256_srai_epi32(x8,SHIFT_EMT_V);                                   \
    x0 = _mm256_packs_epi32(x0,x8);                                            \
    _mm256_store_si256((__m256i *)&dst[0],x0);                                 \

void FUNC(emt_idct_VIII_4x4_v_avx2)(int16_t * restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max)
{
    //const __m256i max  = _mm256_set1_epi16(clip_max);
    //const __m256i min  = _mm256_set1_epi16(clip_min);
    //int shift = (EMT_TRANSFORM_MATRIX_SHIFT + log2_transform_range - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;
    //transpose_per_cg();
    const __m256i add  = _mm256_set1_epi32(ADD_EMT_V);

    __m256i x0,x4,x8,x12,_src2;
    __m256i _src =_mm256_load_si256((__m256i *)src);
    const __m256i dct_matrix = _mm256_setr_epi16(336,  296,  219,  117 ,
                                                 296,    0, -296, -296,
                                                 219, -296, -117,  336,
                                                 117, -296,  336, -219);

    //__m256i _src=_mm256_setr_epi16(1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1);
    //__m256i _src=_mm256_setr_epi16(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15);

    //SWIZZLE_V
    const __m256i _D0 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b01010000);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[0]);
    const __m256i _D1 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b11111010);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[1]);
    const __m256i _D2 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b01010000);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[2]);
    const __m256i _D3 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b11111010);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[3]);

    //  debug4X4_16(_src);

    x0    = _mm256_permute4x64_epi64(_src,0b10110001);

    _src  = _mm256_unpacklo_epi16(_src,x0);
    _src2 = _mm256_permute4x64_epi64(_src,0b01001110);

    //COMPUTE_V
    x0  = _mm256_madd_epi16(_src,  _D0);  //line 0
    x4  = _mm256_madd_epi16(_src2, _D1);  //line 1
    x8  = _mm256_madd_epi16(_src,  _D2);  //line 2
    x12 = _mm256_madd_epi16(_src2, _D3);  //line 3

    x0 =  _mm256_add_epi32(x0,x4);
    x8 =  _mm256_add_epi32(x8,x12);

    //CLIP_V
    x0 =  _mm256_add_epi32(x0,add);
    x8 =  _mm256_add_epi32(x8,add);

    x0 =  _mm256_srai_epi32(x0,SHIFT_EMT_V);
    x8 =  _mm256_srai_epi32(x8,SHIFT_EMT_V);

    x0 = _mm256_packs_epi32(x0,x8);

    // clip
    //x0 = _mm256_min_epi16(x0,max);
    //x0 = _mm256_max_epi16(x0,min);

    //IMPORTANT the result is already in a transposed state, so we don't need much
    //swizzle when doing horizontal transform
    _mm256_store_si256((__m256i *)&dst[0],x0);

}


#define DO_4X4_H2() \
int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;\
const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));                      \
__m256i _src = _mm256_load_si256((__m256i *)src);                              \
__m256i x0,x4,x8,x12;                                                          \
x0  = _mm256_permute4x64_epi64(_src, 0b00000000);                              \
x4  = _mm256_permute4x64_epi64(_src, 0b01010101);                              \
x8  = _mm256_permute4x64_epi64(_src, 0b10101010);                              \
x12 = _mm256_permute4x64_epi64(_src, 0b11111111);                              \
x0  = _mm256_madd_epi16(x0,dct_matrix);                                        \
x4  = _mm256_madd_epi16(x4,dct_matrix);                                        \
x8  = _mm256_madd_epi16(x8,dct_matrix);                                        \
x12 = _mm256_madd_epi16(x12,dct_matrix);                                       \
x0  = _mm256_hadd_epi32(x0,x4);                                                \
x0  = _mm256_permute4x64_epi64(x0, 0b11011000);                                \
x8  = _mm256_hadd_epi32(x8,x12);                                               \
x8  = _mm256_permute4x64_epi64(x8, 0b11011000);                                \
x0  = _mm256_add_epi32(x0,add);                                                \
x8  = _mm256_add_epi32(x8,add);                                                \
x0  = _mm256_srai_epi32(x0,shift);                                             \
x8  = _mm256_srai_epi32(x8,shift);                                             \
x0  = _mm256_packs_epi32(x0,x8);                                               \
x0  = _mm256_permute4x64_epi64(x0, 0b11011000);                                \
_mm256_store_si256((__m256i *)&dst[0],x0);                                     \


#define DO_4X4_H()\
int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;\
const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));                      \
__m256i _src = _mm256_load_si256((__m256i *)src);                              \
__m256i x0,x4,x8,x12;                                                          \
__m256i _D0 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,2,5,7,0,2,5,7));\
__m256i _D2 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(1,3,4,6,1,3,4,6));\
x0  = _mm256_unpacklo_epi32(_src, _src);                                       \
x4  = _mm256_permute4x64_epi64(x0, 0b10110001);                                \
x8  = _mm256_unpackhi_epi32(_src, _src);                                       \
x12 = _mm256_permute4x64_epi64(x8, 0b10110001);                                \
x0  = _mm256_madd_epi16(x0,_D0);                                               \
x4  = _mm256_madd_epi16(x4,_D2);                                               \
x8  = _mm256_madd_epi16(x8,_D0);                                               \
x12 = _mm256_madd_epi16(x12,_D2);                                              \
x0  = _mm256_add_epi32(x0,x4);                                                 \
x8  = _mm256_add_epi32(x8,x12);                                                \
x0  = _mm256_add_epi32(x0,add);                                                \
x8  = _mm256_add_epi32(x8,add);                                                \
x0  = _mm256_srai_epi32(x0,shift);                                             \
x8  = _mm256_srai_epi32(x8,shift);                                             \
x0  = _mm256_packs_epi32(x0,x8);                                               \
_mm256_store_si256((__m256i *)&dst[0],x0);                                     \


//TODO most of the time transform range is 15 and so are clipping values we might want to hard code those values for further
// optimizations
void FUNC(emt_idct_VIII_4x4_h_avx2)(int16_t * restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max)
{
    const __m256i   dct_matrix =_mm256_setr_epi16(336,  296,  219,  117 ,
                                                  296,    0, -296, -296,
                                                  219, -296, -117,  336,
                                                  117, -296,  336, -219);
    int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;
    const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));

    __m256i _src = _mm256_load_si256((__m256i *)src);

    __m256i x0,x4,x8,x12;

    x0 = _mm256_permute4x64_epi64(_src, 0b00000000);
    x4 = _mm256_permute4x64_epi64(_src, 0b01010101);
    x8 = _mm256_permute4x64_epi64(_src, 0b10101010);
    x12 = _mm256_permute4x64_epi64(_src, 0b11111111);
    //  debug4X4_16(x0);
    //  debug4X4_16(x4);
    //  debug4X4_16(x8);
    //  debug4X4_16(x12);

    x0  = _mm256_madd_epi16(x0,dct_matrix);
    x4  = _mm256_madd_epi16(x4,dct_matrix);
    x8  = _mm256_madd_epi16(x8,dct_matrix);
    x12 = _mm256_madd_epi16(x12,dct_matrix);
    //    debug_32(x0);
    //    debug_32(x4);
    //    debug_32(x8);
    //    debug_32(x12);

    x0 = _mm256_hadd_epi32(x0,x4);
    x0 = _mm256_permute4x64_epi64(x0, 0b11011000);
    x8 = _mm256_hadd_epi32(x8,x12);
    x8 = _mm256_permute4x64_epi64(x8, 0b11011000);
    //    debug_32(x0);
    //    debug_32(x8);

    x0 =  _mm256_add_epi32(x0,add);
    x8 =  _mm256_add_epi32(x8,add);
    //    debug_32(x0);
    //    debug_32(x8);

    x0 =  _mm256_srai_epi32(x0,shift);
    x8 =  _mm256_srai_epi32(x8,shift);
    //    debug_32(x0);
    //    debug_32(x8);

    x0 = _mm256_packs_epi32(x0,x8);

    //FIXME we might be able to avoid this permutation.
    x0 = _mm256_permute4x64_epi64(x0, 0b11011000);

    _mm256_store_si256((__m256i *)&dst[0],x0);
}

void FUNC(emt_idct_V_4x4_v_avx2)(int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max){
    const __m256i dct_matrix = _mm256_setr_epi16(194,  274,  274,  274,
                                                 274,  241,  -86, -349,
                                                 274,  -86, -349,  241,
                                                 274, -349,  241,  -86);
    DO_4X4_V();
//    VAR_DECL_AND_LOAD4X4_AVX2_V();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_V();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}

void FUNC(emt_idct_V_4x4_h_avx2)(int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max){
    const __m256i dct_matrix = _mm256_setr_epi16(194,  274,  274,  274,
                                                 274,  241,  -86, -349,
                                                 274,  -86, -349,  241,
                                                 274, -349,  241,  -86);
    DO_4X4_H();
//    VAR_DECL_AND_LOAD4X4_AVX2_H();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_H();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}


void FUNC(emt_idct_II_4x4_v_avx2)(int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max){
    const __m256i dct_matrix = _mm256_setr_epi16(256,  334,  256,  139 ,
                                                 256,  139, -256, -334 ,
                                                 256, -139, -256,  334 ,
                                                 256, -334,  256, -139);
    DO_4X4_V();
//    VAR_DECL_AND_LOAD4X4_AVX2_V();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_V();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}

void FUNC(emt_idct_II_4x4_h_avx2)(int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max){
    const __m256i dct_matrix = _mm256_setr_epi16(256,  334,  256,  139 ,
                                                 256,  139, -256, -334 ,
                                                 256, -139, -256,  334 ,
                                                 256, -334,  256, -139);
    DO_4X4_H();
//    VAR_DECL_AND_LOAD4X4_AVX2_H();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_H();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}

void FUNC(emt_idct_I_4x4_v_avx2)(int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max){
    const __m256i dct_matrix = _mm256_setr_epi16(190,  308,  308,  190 ,
                                                 308,  190, -190, -308 ,
                                                 308, -190, -190,  308 ,
                                                 190, -308,  308, -190);
    DO_4X4_V();
//    VAR_DECL_AND_LOAD4X4_AVX2_V();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_V();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}

void FUNC(emt_idct_I_4x4_h_avx2)(int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max){
    const __m256i dct_matrix = _mm256_setr_epi16(190,  308,  308,  190 ,
                                                 308,  190, -190, -308 ,
                                                 308, -190, -190,  308 ,
                                                 190, -308,  308, -190);
    DO_4X4_H();
//    VAR_DECL_AND_LOAD4X4_AVX2_H();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_H();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}

void FUNC(emt_idct_VII_4x4_v_avx2) (int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max){
    const __m256i dct_matrix = _mm256_setr_epi16(117,  296,  336,  219 ,
                                                 219,  296, -117, -336 ,
                                                 296,    0, -296,  296 ,
                                                 336, -296,  219, -117);
    DO_4X4_V();
//    VAR_DECL_AND_LOAD4X4_AVX2_V();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_V();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}

void FUNC(emt_idct_VII_4x4_h_avx2)(int16_t *restrict src, int16_t * restrict dst, int log2_transform_range, const int clip_min, const int clip_max)
{
    const __m256i dct_matrix = _mm256_setr_epi16 (117,  296,  336,  219 ,
                                                  219,  296, -117, -336 ,
                                                  296,    0, -296,  296 ,
                                                  336, -296,  219, -117);
    DO_4X4_H();
//    VAR_DECL_AND_LOAD4X4_AVX2_H();
//    TRANSPOSE_4X4_AVX2();
//    COMPUTE4X4_AVX2();
//    ROUND_4X4_AVX2_H();
//    PACK_CLIP_AND_STORE_4X4_AVX2();
}

//______________________________________________________________________________
//


DECLARE_ALIGNED(32, static const int16_t, Id_8x8v2[4*16]) =
{
      1,  0,  0,  0,  0,  1,  0,  0,  0, 0 , 1, 0, 0, 0, 0,  1 ,
      0,  0,  0,  0,  0,  0,  0,  0,  0, 0 , 0, 0, 0, 0, 0,  0 ,
      0,  0,  0,  0,  0,  0,  0,  0,  0, 0 , 0, 0, 0, 0, 0,  0 ,
      1,  0,  0,  0,  0,  1,  0,  0,  0, 0 , 1, 0, 0, 0, 0,  1 ,
};

#define IDCT8X8_V(DCT_type,DCT_num)\
void FUNC(emt_idct_##DCT_num##_8x8_v_avx2)(int16_t * src, int16_t * dst,  int *significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)\
{                                                                              \
    const __m256i add  = _mm256_set1_epi32(ADD_EMT_V);                         \
    int i,j,k;                                                                 \
    __m256i x0,x4,x8,x12,_src2;                                                \
    for(i = 0; i < 2; i++){                                                    \
        for (j = 0; j < 2; j++){                                               \
            __m256i x0_tmp = _mm256_setzero_si256();                           \
            __m256i x8_tmp = _mm256_setzero_si256();                           \
            for (k = 0; k < 2; k++ ){                                          \
    if(((uint8_t *)significant_cg_list)[2*k+i]){                               \
                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(2*k+i)]);  \
                __m256i dct_matrix = _mm256_load_si256((__m256i *) TR_##DCT_type##_8x8_per_CG[2*j+k]);\
                const __m256i _D0 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b01010000);\
                const __m256i _D1 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b11111010);\
                const __m256i _D2 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b01010000);\
                const __m256i _D3 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b11111010);\
                x0    = _mm256_permute4x64_epi64(_src,0b10110001);             \
                _src  = _mm256_unpacklo_epi16(_src,x0);                        \
                _src2 = _mm256_permute4x64_epi64(_src,0b01001110);             \
                x0  = _mm256_madd_epi16(_src,  _D0);                           \
                x4  = _mm256_madd_epi16(_src2, _D1);                           \
                x8  = _mm256_madd_epi16(_src,  _D2);                           \
                x12 = _mm256_madd_epi16(_src2, _D3);                           \
                x0 =  _mm256_add_epi32(x0,x4);                                 \
                x8 =  _mm256_add_epi32(x8,x12);                                \
                x0_tmp = _mm256_add_epi32(x0,x0_tmp);                          \
                x8_tmp = _mm256_add_epi32(x8,x8_tmp);                          \
    }                                                                          \
            }                                                                  \
            x0 =  _mm256_add_epi32(x0_tmp,add);                                \
            x8 =  _mm256_add_epi32(x8_tmp,add);                                \
            x0 =  _mm256_srai_epi32(x0,SHIFT_EMT_V);                           \
            x8 =  _mm256_srai_epi32(x8,SHIFT_EMT_V);                           \
            x0 =  _mm256_packs_epi32(x0,x8);                                   \
            _mm256_store_si256((__m256i *)&dst[(2*j+i)*16],x0);                \
        }                                                                      \
    }                                                                          \
}                                                                              \


#define IDCT8X8_H(DCT_type,DCT_num)\
void FUNC(emt_idct_##DCT_num##_8x8_h_avx2)(int16_t * restrict src, int16_t * restrict dst,  int * restrict significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)\
{                                                                              \
    const int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;\
    const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));                  \
    __m256i CG[4];                                                             \
    int i,j,k;                                                                 \
    __m256i x0,x4,x8,x12;                                                      \
    for(i = 0; i < 2; i++){                                                    \
        for (j = 0; j < 2; j++){                                               \
            __m256i x0_tmp = _mm256_setzero_si256();                           \
            __m256i x8_tmp = _mm256_setzero_si256();                           \
            for (k = 0; k < 2 /*&& significant_cg_list[k] < j*2*/; k++ ){      \
                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(2*i+k)]);  \
                __m256i dct_matrix = _mm256_load_si256((__m256i *) TR_##DCT_type##_8x8_per_CG[2*j+k]);\
                __m256i _D0 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,2,5,7,0,2,5,7));\
                __m256i _D2 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(1,3,4,6,1,3,4,6));\
                x0 = _mm256_unpacklo_epi32(_src, _src);                        \
                x4 = _mm256_permute4x64_epi64(x0, 0b10110001);                 \
                x8 = _mm256_unpackhi_epi32(_src, _src);                        \
                x12 = _mm256_permute4x64_epi64(x8, 0b10110001);                \
                x0  = _mm256_madd_epi16(x0,_D0);                               \
                x4  = _mm256_madd_epi16(x4,_D2);                               \
                x8  = _mm256_madd_epi16(x8,_D0);                               \
                x12 = _mm256_madd_epi16(x12,_D2);                              \
                x0 = _mm256_add_epi32(x0,x4);                                  \
                x8 = _mm256_add_epi32(x8,x12);                                 \
                x0_tmp = _mm256_add_epi32(x0,x0_tmp);                          \
                x8_tmp = _mm256_add_epi32(x8,x8_tmp);                          \
            }                                                                  \
            x0 =  _mm256_add_epi32(x0_tmp,add);                                \
            x8 =  _mm256_add_epi32(x8_tmp,add);                                \
            x0 =  _mm256_srai_epi32(x0,shift);                                 \
            x8 =  _mm256_srai_epi32(x8,shift);                                 \
            x0 = _mm256_packs_epi32(x0,x8);                                    \
            CG[2*i+j]= x0;                                                     \
       }                                                                       \
    }                                                                          \
    {                                                                          \
    __m256i _dst1 = _mm256_permute2x128_si256(CG[0],CG[1],0b00100000);         \
    __m256i _dst2 = _mm256_permute2x128_si256(CG[0],CG[1],0b00110001);         \
    _dst1 = _mm256_permute4x64_epi64(_dst1,0b11011000);                        \
    _dst2 = _mm256_permute4x64_epi64(_dst2,0b11011000);                        \
    _mm256_store_si256((__m256i *)&dst[0],_dst1);                              \
    _mm256_store_si256((__m256i *)&dst[1*16],_dst2);                           \
    _dst1 = _mm256_permute2x128_si256(CG[2],CG[3],0b00100000);                 \
    _dst2 = _mm256_permute2x128_si256(CG[2],CG[3],0b00110001);                 \
    _dst1 = _mm256_permute4x64_epi64(_dst1,0b11011000);                        \
    _dst2 = _mm256_permute4x64_epi64(_dst2,0b11011000);                        \
    _mm256_store_si256((__m256i *)&dst[2*16],_dst1);                           \
    _mm256_store_si256((__m256i *)&dst[3*16],_dst2);                           \
    }                                                                          \
    }                                                                          \


IDCT8X8_V(DCT_II,II)
IDCT8X8_V(DCT_V,V)
IDCT8X8_V(DCT_VIII,VIII)
IDCT8X8_V(DST_VII,VII)
IDCT8X8_V(DST_I,I)


IDCT8X8_H(DCT_II,II)
IDCT8X8_H(DCT_V,V)
IDCT8X8_H(DCT_VIII,VIII)
IDCT8X8_H(DST_VII,VII)
IDCT8X8_H(DST_I,I)


//void FUNC(emt_idct_V_8x8_v_avx2)(int16_t * src, int16_t * dst,  int *significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)
//{
//    const __m256i add  = _mm256_set1_epi32(ADD_EMT_V);

//    int i,j,k;
//    __m256i x0,x4,x8,x12,_src2;

//    for(i = 0; i < 2; i++){
//        for (j = 0; j < 2; j++){
//            __m256i x0_tmp = _mm256_setzero_si256();
//            __m256i x8_tmp = _mm256_setzero_si256();

//            for (k = 0; k < 2; k++ ){
//                if(((uint8_t *)significant_cg_list)[2*k+i]){
//                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(2*k+i)]);
//                __m256i dct_matrix = _mm256_load_si256((__m256i *)DCT_V_8x8v2[2*j+k]);

//                //SWIZZLE_V
//                const __m256i _D0 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b01010000);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[0]);
//                const __m256i _D1 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b11111010);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[1]);
//                const __m256i _D2 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b01010000);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[2]);
//                const __m256i _D3 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b11111010);//_mm256_load_si256((__m256i *)DCT_VIII_4x4v2[3]);

//                x0    = _mm256_permute4x64_epi64(_src,0b10110001);
//                _src  = _mm256_unpacklo_epi16(_src,x0);
//                _src2 = _mm256_permute4x64_epi64(_src,0b01001110);

//                //COMPUTE_V
//                x0  = _mm256_madd_epi16(_src,  _D0);  //line 0
//                x4  = _mm256_madd_epi16(_src2, _D1);  //line 1
//                x8  = _mm256_madd_epi16(_src,  _D2);  //line 2
//                x12 = _mm256_madd_epi16(_src2, _D3);  //line 3

//                x0 =  _mm256_add_epi32(x0,x4);
//                x8 =  _mm256_add_epi32(x8,x12);

//                x0_tmp = _mm256_add_epi32(x0,x0_tmp);
//                x8_tmp = _mm256_add_epi32(x8,x8_tmp);
//                //debug_32(x0_tmp);
//                //debug_32(x8_tmp);
//            }
//            }
//            //CLIP_V
//            x0 =  _mm256_add_epi32(x0_tmp,add);
//            x8 =  _mm256_add_epi32(x8_tmp,add);

//            x0 =  _mm256_srai_epi32(x0,SHIFT_EMT_V);
//            x8 =  _mm256_srai_epi32(x8,SHIFT_EMT_V);

//            //x0 = _mm256_packs_epi32(x0,x8);
//            x0 = _mm256_packs_epi32(x0,x8);

//            //debug4X4_16(x0);

//            //DEBUG
//            //            fprintf(stderr,"STORE i %d , j %d ,%d \n",i,j,(2*i+j)*16);
//            //            for (m = 0; m < 8 ; m++){
//            //                    for (n = 0; n < 8; n++){
//            //                        printf("%*d ",5,dst[m*8+n]);
//            //                    }
//            //                    printf("\n");
//            //                }
//            //            fprintf(stderr,"store\n\n");

//            //STORE
//            _mm256_store_si256((__m256i *)&dst[(2*j+i)*16],x0);

//            //DEBUG
//            //            for (m = 0; m < 8 ; m++){
//            //                    for (n = 0; n < 8; n++){
//            //                        printf("%*d ",5,dst[m*8+n]);
//            //                    }
//            //                    printf("\n");
//            //                }
//            //            fprintf(stderr,"end\n\n");
//        }
//    }
//    //IMPORTANT the result is already in a transposed state, so we don't need much
//    //swizzle when doing horizontal transform
//}

//void FUNC(emt_idct_V_8x8_h_avx2)(int16_t * restrict src, int16_t * restrict dst,  int * restrict significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)
//{
//    const int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;
//    const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));

//    __m256i test[4];
//    //DECLARE_ALIGNED(32, int16_t, tmp[8 * 8])={0};

//    int i,j,k;
//    __m256i x0,x4,x8,x12;

//    for(i = 0; i < 2; i++){
//        for (j = 0; j < 2; j++){
//            __m256i x0_tmp = _mm256_setzero_si256();
//            __m256i x8_tmp = _mm256_setzero_si256();
//            for (k = 0; k < 2 /*&& significant_cg_list[k] < j*2*/; k++ ){

//                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(2*i+k)]);

//                __m256i dct_matrix = _mm256_load_si256((__m256i *)DCT_V_8x8v2[2*j+k]);
//                __m256i _D0 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,2,5,7,0,2,5,7));\
//                __m256i _D2 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(1,3,4,6,1,3,4,6));\

//                x0 = _mm256_unpacklo_epi32(_src, _src);
//                x4 = _mm256_permute4x64_epi64(x0, 0b10110001);
//                x8 = _mm256_unpackhi_epi32(_src, _src);
//                x12 = _mm256_permute4x64_epi64(x8, 0b10110001);

//                x0  = _mm256_madd_epi16(x0,_D0);
//                x4  = _mm256_madd_epi16(x4,_D2);
//                x8  = _mm256_madd_epi16(x8,_D0);
//                x12 = _mm256_madd_epi16(x12,_D2);

//                x0 = _mm256_add_epi32(x0,x4);
//                x8 = _mm256_add_epi32(x8,x12);

//                x0_tmp = _mm256_add_epi32(x0,x0_tmp);
//                x8_tmp = _mm256_add_epi32(x8,x8_tmp);
//                //debug_32(x0_tmp);
//                //debug_32(x8_tmp);
//            }
//            //CLIP_V

//            x0 =  _mm256_add_epi32(x0_tmp,add);
//            x8 =  _mm256_add_epi32(x8_tmp,add);

//            x0 =  _mm256_srai_epi32(x0,shift);
//            x8 =  _mm256_srai_epi32(x8,shift);

//            x0 = _mm256_packs_epi32(x0,x8);

//            test[2*i+j]= x0;


//            //DEBUG
////            fprintf(stderr,"STORE i %d , j %d ,%d \n",i,j,(2*i+j)*16);
////            for (m = 0; m < 8 ; m++){
////                    for (n = 0; n < 8; n++){
////                        printf("%*d ",5,dst[m*8+n]);
////                    }
////                    printf("\n");
////                }
////            fprintf(stderr,"store\n\n");

//            //STORE
//            //_mm256_store_si256((__m256i *)&tmp[(2*i+j)*16],x0);

//            //DEBUG
////            for (m = 0; m < 8 ; m++){
////                    for (n = 0; n < 8; n++){
////                        //dst[m*8+n]=tmp[[m*8+n]]
////                        fprintf(stderr,"%*d ",5,tmp[m*8+n]);
////                    }
////                    fprintf(stderr,"\n");
////                }
////            fprintf(stderr,"end\n\n");
//       }
//    }
//    // clip
//    //x0 = _mm256_min_epi16(x0,max);
//    //x0 = _mm256_max_epi16(x0,min);

//    //IMPORTANT the result is already in a transposed state, so we don't need much
//    //swizzle when doing horizontal transform
//    //_mm256_store_si256((__m256i *)&dst[0],x0);
//    __m256i _dst1 = _mm256_permute2x128_si256(test[0],test[1],0b00100000);
//    __m256i _dst2 = _mm256_permute2x128_si256(test[0],test[1],0b00110001);

//    _dst1 = _mm256_permute4x64_epi64(_dst1,0b11011000);
//    _dst2 = _mm256_permute4x64_epi64(_dst2,0b11011000);

//    _mm256_store_si256((__m256i *)&dst[0],_dst1);
//    _mm256_store_si256((__m256i *)&dst[1*16],_dst2);

//    _dst1 = _mm256_permute2x128_si256(test[2],test[3],0b00100000);
//    _dst2 = _mm256_permute2x128_si256(test[2],test[3],0b00110001);

//    _dst1 = _mm256_permute4x64_epi64(_dst1,0b11011000);
//    _dst2 = _mm256_permute4x64_epi64(_dst2,0b11011000);

//    _mm256_store_si256((__m256i *)&dst[2*16],_dst1);
//    _mm256_store_si256((__m256i *)&dst[3*16],_dst2);
//}



//______________________________________________________________________________
//16x16


#define IDCT16x16_V(DCT_type,DCT_num)\
void FUNC(emt_idct_##DCT_num##_16x16_v_avx2)(int16_t * src, int16_t * dst,  int *significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)\
{                                                                              \
    const __m256i add  = _mm256_set1_epi32(ADD_EMT_V);                         \
    int i,j,k;                                                                 \
    __m256i x0,x4,x8,x12,_src2;                                                \
    for(i = 0; i < 4; i++){                                                    \
        for (j = 0; j < 4; j++){                                               \
            __m256i x0_tmp = _mm256_setzero_si256();                           \
            __m256i x8_tmp = _mm256_setzero_si256();                           \
            for (k = 0; k < 4; k++ ){                                          \
    if(((uint8_t *)significant_cg_list)[4*k+i]){                               \
                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(4*k+i)]);  \
                __m256i dct_matrix = _mm256_load_si256((__m256i *) TR_##DCT_type##_16x16_per_CG[4*j+k]);\
                const __m256i _D0 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b01010000);\
                const __m256i _D1 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b11111010);\
                const __m256i _D2 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b01010000);\
                const __m256i _D3 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b11111010);\
                x0    = _mm256_permute4x64_epi64(_src,0b10110001);             \
                _src  = _mm256_unpacklo_epi16(_src,x0);                        \
                _src2 = _mm256_permute4x64_epi64(_src,0b01001110);             \
                x0  = _mm256_madd_epi16(_src,  _D0);                           \
                x4  = _mm256_madd_epi16(_src2, _D1);                           \
                x8  = _mm256_madd_epi16(_src,  _D2);                           \
                x12 = _mm256_madd_epi16(_src2, _D3);                           \
                x0 =  _mm256_add_epi32(x0,x4);                                 \
                x8 =  _mm256_add_epi32(x8,x12);                                \
                x0_tmp = _mm256_add_epi32(x0,x0_tmp);                          \
                x8_tmp = _mm256_add_epi32(x8,x8_tmp);                          \
    }                                                                          \
            }                                                                  \
            x0 =  _mm256_add_epi32(x0_tmp,add);                                \
            x8 =  _mm256_add_epi32(x8_tmp,add);                                \
            x0 =  _mm256_srai_epi32(x0,SHIFT_EMT_V);                           \
            x8 =  _mm256_srai_epi32(x8,SHIFT_EMT_V);                           \
            x0 =  _mm256_packs_epi32(x0,x8);                                   \
            _mm256_store_si256((__m256i *)&dst[(4*j+i)*16],x0);                \
        }                                                                      \
    }                                                                          \
}                                                                              \


#define IDCT16x16_H(DCT_type,DCT_num)\
void FUNC(emt_idct_##DCT_num##_16x16_h_avx2)(int16_t * restrict src, int16_t * restrict dst,  int * restrict significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)\
{                                                                              \
    const int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;\
    const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));                  \
    __m256i CG[16];                                                            \
    int i,j,k;                                                                 \
    __m256i x0,x4,x8,x12;                                                      \
    for(i = 0; i < 4; i++){                                                    \
        for (j = 0; j < 4; j++){                                               \
            __m256i x0_tmp = _mm256_setzero_si256();                           \
            __m256i x8_tmp = _mm256_setzero_si256();                           \
            for (k = 0; k < 4 /*&& significant_cg_list[k] < j*2*/; k++ ){      \
                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(4*i+k)]);  \
                __m256i dct_matrix = _mm256_load_si256((__m256i *) TR_##DCT_type##_16x16_per_CG[4*j+k]);\
                __m256i _D0 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,2,5,7,0,2,5,7));\
                __m256i _D2 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(1,3,4,6,1,3,4,6));\
                x0 = _mm256_unpacklo_epi32(_src, _src);                        \
                x4 = _mm256_permute4x64_epi64(x0, 0b10110001);                 \
                x8 = _mm256_unpackhi_epi32(_src, _src);                        \
                x12 = _mm256_permute4x64_epi64(x8, 0b10110001);                \
                x0  = _mm256_madd_epi16(x0,_D0);                               \
                x4  = _mm256_madd_epi16(x4,_D2);                               \
                x8  = _mm256_madd_epi16(x8,_D0);                               \
                x12 = _mm256_madd_epi16(x12,_D2);                              \
                x0 = _mm256_add_epi32(x0,x4);                                  \
                x8 = _mm256_add_epi32(x8,x12);                                 \
                x0_tmp = _mm256_add_epi32(x0,x0_tmp);                          \
                x8_tmp = _mm256_add_epi32(x8,x8_tmp);                          \
            }                                                                  \
            x0 =  _mm256_add_epi32(x0_tmp,add);                                \
            x8 =  _mm256_add_epi32(x8_tmp,add);                                \
            x0 =  _mm256_srai_epi32(x0,shift);                                 \
            x8 =  _mm256_srai_epi32(x8,shift);                                 \
            x0 = _mm256_packs_epi32(x0,x8);                                    \
            CG[4*i+j]= x0;                                                     \
       }                                                                       \
    }                                                                          \
for(i=0;i<4; i++){                                                             \
    __m256i dst_tmp;                                                           \
    __m256i _dst1 = _mm256_permute2x128_si256(CG[4*i+0],CG[4*i+1],0b00100000); \
    __m256i _dst2 = _mm256_permute2x128_si256(CG[4*i+0],CG[4*i+1],0b00110001); \
    __m256i _dst3 = _mm256_permute2x128_si256(CG[4*i+2],CG[4*i+3],0b00100000); \
    __m256i _dst4 = _mm256_permute2x128_si256(CG[4*i+2],CG[4*i+3],0b00110001); \
    _dst1 = _mm256_permute4x64_epi64(_dst1,0b11011000);                        \
    _dst2 = _mm256_permute4x64_epi64(_dst2,0b11011000);                        \
    _dst3 = _mm256_permute4x64_epi64(_dst3,0b11011000);                        \
    _dst4 = _mm256_permute4x64_epi64(_dst4,0b11011000);                        \
    dst_tmp = _mm256_permute2x128_si256(_dst1, _dst3,0b00100000);              \
    _mm256_store_si256((__m256i *)&dst[16*4*i+0],dst_tmp);                     \
    dst_tmp = _mm256_permute2x128_si256(_dst1, _dst3,0b00110001);              \
    _mm256_store_si256((__m256i *)&dst[16*4*i+1*16],dst_tmp);                  \
    dst_tmp = _mm256_permute2x128_si256(_dst2, _dst4, 0b00100000);             \
    _mm256_store_si256((__m256i *)&dst[16*4*i+2*16],dst_tmp);                  \
    dst_tmp = _mm256_permute2x128_si256(_dst2, _dst4, 0b00110001);             \
    _mm256_store_si256((__m256i *)&dst[16*4*i+3*16],dst_tmp);                  \
}                                                                              \
    }                                                                          \


IDCT16x16_V(DCT_II,II)
IDCT16x16_V(DCT_V,V)
IDCT16x16_V(DCT_VIII,VIII)
IDCT16x16_V(DST_VII,VII)
IDCT16x16_V(DST_I,I)

IDCT16x16_H(DCT_II,II)
IDCT16x16_H(DCT_V,V)
IDCT16x16_H(DCT_VIII,VIII)
IDCT16x16_H(DST_VII,VII)
IDCT16x16_H(DST_I,I)

//______________________________________________________________________________
//16x16


#define IDCT32x32_V(DCT_type,DCT_num)\
void FUNC(emt_idct_##DCT_num##_32x32_v_avx2)(int16_t * src, int16_t * dst,  int *significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)\
{                                                                              \
    const __m256i add  = _mm256_set1_epi32(ADD_EMT_V);                         \
    int i,j,k;                                                                 \
    __m256i x0,x4,x8,x12,_src2;                                                \
    for(i = 0; i < 8; i++){                                                    \
        for (j = 0; j < 8; j++){                                               \
            __m256i x0_tmp = _mm256_setzero_si256();                           \
            __m256i x8_tmp = _mm256_setzero_si256();                           \
            for (k = 0; k < 8; k++ ){                                          \
    if(((uint8_t *)significant_cg_list)[8*k+i]){                               \
                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(8*k+i)]);  \
                __m256i dct_matrix = _mm256_load_si256((__m256i *) TR_##DCT_type##_32x32_per_CG[8*j+k]);\
                const __m256i _D0 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b01010000);\
                const __m256i _D1 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,0,5,5,1,1,4,4)),0b11111010);\
                const __m256i _D2 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b01010000);\
                const __m256i _D3 =_mm256_permute4x64_epi64(_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(2,2,7,7,3,3,6,6)),0b11111010);\
                x0    = _mm256_permute4x64_epi64(_src,0b10110001);             \
                _src  = _mm256_unpacklo_epi16(_src,x0);                        \
                _src2 = _mm256_permute4x64_epi64(_src,0b01001110);             \
                x0  = _mm256_madd_epi16(_src,  _D0);                           \
                x4  = _mm256_madd_epi16(_src2, _D1);                           \
                x8  = _mm256_madd_epi16(_src,  _D2);                           \
                x12 = _mm256_madd_epi16(_src2, _D3);                           \
                x0 =  _mm256_add_epi32(x0,x4);                                 \
                x8 =  _mm256_add_epi32(x8,x12);                                \
                x0_tmp = _mm256_add_epi32(x0,x0_tmp);                          \
                x8_tmp = _mm256_add_epi32(x8,x8_tmp);                          \
    }                                                                          \
            }                                                                  \
            x0 =  _mm256_add_epi32(x0_tmp,add);                                \
            x8 =  _mm256_add_epi32(x8_tmp,add);                                \
            x0 =  _mm256_srai_epi32(x0,SHIFT_EMT_V);                           \
            x8 =  _mm256_srai_epi32(x8,SHIFT_EMT_V);                           \
            x0 =  _mm256_packs_epi32(x0,x8);                                   \
            _mm256_store_si256((__m256i *)&dst[(8*j+i)*16],x0);                \
        }                                                                      \
    }                                                                          \
}                                                                              \


#define IDCT32x32_H(DCT_type,DCT_num)\
void FUNC(emt_idct_##DCT_num##_32x32_h_avx2)(int16_t * restrict src, int16_t * restrict dst,  int * restrict significant_cg_list, int log2_transform_range, const int clip_min, const int clip_max)\
{                                                                              \
    const int shift = (EMT_TRANSFORM_MATRIX_SHIFT + 15 - 1) - BIT_DEPTH + COM16_C806_TRANS_PREC;\
    const __m256i add  = _mm256_set1_epi32(1 << (shift - 1));                  \
    __m256i CG[64];                                                            \
    int i,j,k;                                                                 \
    __m256i x0,x4,x8,x12;                                                      \
    for(i = 0; i < 8; i++){                                                    \
        for (j = 0; j < 8; j++){                                               \
            __m256i x0_tmp = _mm256_setzero_si256();                           \
            __m256i x8_tmp = _mm256_setzero_si256();                           \
            for (k = 0; k < 8 /*&& significant_cg_list[k] < j*2*/; k++ ){      \
                __m256i _src =_mm256_load_si256((__m256i *)&src[16*(8*i+k)]);  \
                __m256i dct_matrix = _mm256_load_si256((__m256i *) TR_##DCT_type##_32x32_per_CG[8*j+k]);\
                __m256i _D0 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(0,2,5,7,0,2,5,7));\
                __m256i _D2 =_mm256_permutevar8x32_epi32(dct_matrix,_mm256_setr_epi32(1,3,4,6,1,3,4,6));\
                x0 = _mm256_unpacklo_epi32(_src, _src);                        \
                x4 = _mm256_permute4x64_epi64(x0, 0b10110001);                 \
                x8 = _mm256_unpackhi_epi32(_src, _src);                        \
                x12 = _mm256_permute4x64_epi64(x8, 0b10110001);                \
                x0  = _mm256_madd_epi16(x0,_D0);                               \
                x4  = _mm256_madd_epi16(x4,_D2);                               \
                x8  = _mm256_madd_epi16(x8,_D0);                               \
                x12 = _mm256_madd_epi16(x12,_D2);                              \
                x0 = _mm256_add_epi32(x0,x4);                                  \
                x8 = _mm256_add_epi32(x8,x12);                                 \
                x0_tmp = _mm256_add_epi32(x0,x0_tmp);                          \
                x8_tmp = _mm256_add_epi32(x8,x8_tmp);                          \
            }                                                                  \
            x0 =  _mm256_add_epi32(x0_tmp,add);                                \
            x8 =  _mm256_add_epi32(x8_tmp,add);                                \
            x0 =  _mm256_srai_epi32(x0,shift);                                 \
            x8 =  _mm256_srai_epi32(x8,shift);                                 \
            x0 = _mm256_packs_epi32(x0,x8);                                    \
            CG[8*i+j]= x0;                                                     \
       }                                                                       \
    }                                                                          \
for(i=0;i < 8; i++){                                                           \
    __m256i dst_tmp;                                                           \
    __m256i _dst1 = _mm256_permute2x128_si256(CG[8*i+0],CG[8*i+1],0b00100000); \
    __m256i _dst2 = _mm256_permute2x128_si256(CG[8*i+0],CG[8*i+1],0b00110001); \
    __m256i _dst3 = _mm256_permute2x128_si256(CG[8*i+2],CG[8*i+3],0b00100000); \
    __m256i _dst4 = _mm256_permute2x128_si256(CG[8*i+2],CG[8*i+3],0b00110001); \
    _dst1 = _mm256_permute4x64_epi64(_dst1,0b11011000);                        \
    _dst2 = _mm256_permute4x64_epi64(_dst2,0b11011000);                        \
    _dst3 = _mm256_permute4x64_epi64(_dst3,0b11011000);                        \
    _dst4 = _mm256_permute4x64_epi64(_dst4,0b11011000);                        \
    dst_tmp = _mm256_permute2x128_si256(_dst1, _dst3,0b00100000);              \
    _mm256_store_si256((__m256i *)&dst[32*4*i+0],dst_tmp);                     \
    dst_tmp = _mm256_permute2x128_si256(_dst1, _dst3,0b00110001);              \
    _mm256_store_si256((__m256i *)&dst[32*4*i+1*32],dst_tmp);                  \
    dst_tmp = _mm256_permute2x128_si256(_dst2, _dst4, 0b00100000);             \
    _mm256_store_si256((__m256i *)&dst[32*4*i+2*32],dst_tmp);                  \
    dst_tmp = _mm256_permute2x128_si256(_dst2, _dst4, 0b00110001);             \
    _mm256_store_si256((__m256i *)&dst[32*4*i+3*32],dst_tmp);                  \
    _dst1 = _mm256_permute2x128_si256(CG[8*i+4],CG[8*i+5],0b00100000);         \
    _dst2 = _mm256_permute2x128_si256(CG[8*i+4],CG[8*i+5],0b00110001);         \
    _dst3 = _mm256_permute2x128_si256(CG[8*i+6],CG[8*i+7],0b00100000);         \
    _dst4 = _mm256_permute2x128_si256(CG[8*i+6],CG[8*i+7],0b00110001);         \
    _dst1 = _mm256_permute4x64_epi64(_dst1,0b11011000);                        \
    _dst2 = _mm256_permute4x64_epi64(_dst2,0b11011000);                        \
    _dst3 = _mm256_permute4x64_epi64(_dst3,0b11011000);                        \
    _dst4 = _mm256_permute4x64_epi64(_dst4,0b11011000);                        \
    dst_tmp = _mm256_permute2x128_si256(_dst1, _dst3,0b00100000);              \
    _mm256_store_si256((__m256i *)&dst[32*4*i+0*32+16],dst_tmp);               \
    dst_tmp = _mm256_permute2x128_si256(_dst1, _dst3,0b00110001);              \
    _mm256_store_si256((__m256i *)&dst[32*4*i+1*32+16],dst_tmp);               \
    dst_tmp = _mm256_permute2x128_si256(_dst2, _dst4, 0b00100000);             \
    _mm256_store_si256((__m256i *)&dst[32*4*i+2*32+16],dst_tmp);               \
    dst_tmp = _mm256_permute2x128_si256(_dst2, _dst4, 0b00110001);             \
    _mm256_store_si256((__m256i *)&dst[32*4*i+3*32+16],dst_tmp);               \
}                                                                              \
    }                                                                          \


IDCT32x32_V(DCT_II,II)
IDCT32x32_V(DCT_V,V)
IDCT32x32_V(DCT_VIII,VIII)
IDCT32x32_V(DST_VII,VII)
IDCT32x32_V(DST_I,I)

IDCT32x32_H(DCT_II,II)
IDCT32x32_H(DCT_V,V)
IDCT32x32_H(DCT_VIII,VIII)
IDCT32x32_H(DST_VII,VII)
IDCT32x32_H(DST_I,I)
